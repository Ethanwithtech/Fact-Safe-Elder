#!/usr/bin/env python3
"""
çœŸå®æ•°æ®é›†ä¸‹è½½å’Œå¤„ç†è„šæœ¬
ä»å…¬å¼€æ•°æ®æºè·å–çœŸå®çš„è™šå‡ä¿¡æ¯æ£€æµ‹æ•°æ®
"""

import os
import json
import requests
from pathlib import Path
from typing import List, Dict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealDatasetDownloader:
    """çœŸå®æ•°æ®é›†ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.base_dir = Path("data/raw")
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.datasets = []
        
    def download_kaggle_fake_news(self):
        """ä¸‹è½½Kaggleå‡æ–°é—»æ•°æ®é›†ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰"""
        logger.info("å‡†å¤‡ä¸‹è½½Kaggleæ•°æ®é›†...")
        
        # è¯´æ˜ï¼šéœ€è¦Kaggle API credentials
        logger.info("""
        è¦ä¸‹è½½Kaggleæ•°æ®é›†ï¼Œè¯·ï¼š
        1. è®¿é—® https://www.kaggle.com/account
        2. åˆ›å»ºAPI Token (ä¼šä¸‹è½½kaggle.json)
        3. å°†kaggle.jsonæ”¾åˆ° ~/.kaggle/
        4. è¿è¡Œ: pip install kaggle
        5. è¿è¡Œ: kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset
        """)
        
    def create_synthetic_realistic_dataset(self) -> int:
        """åˆ›å»ºåŸºäºçœŸå®æ¡ˆä¾‹çš„ç»¼åˆæ•°æ®é›†"""
        logger.info("åˆ›å»ºåŸºäºçœŸå®æ¡ˆä¾‹çš„æ•°æ®é›†...")
        
        # åŸºäºçœŸå®æ–°é—»æ¡ˆä¾‹çš„æ•°æ®é›†
        real_case_dataset = []
        
        # é‡‘èè¯ˆéª—çœŸå®æ¡ˆä¾‹
        financial_scams = [
            {
                "text": "æŸæŠ•èµ„å…¬å¸æ‰¿è¯ºå¹´åŒ–æ”¶ç›Š30%ï¼Œä¿æœ¬ä¿æ¯ï¼Œå®ä¸ºéæ³•é›†èµ„éª—å±€",
                "label": "danger",
                "category": "financial",
                "source": "çœŸå®æ¡ˆä¾‹:eç§Ÿå®éæ³•é›†èµ„æ¡ˆ"
            },
            {
                "text": "è™šæ‹Ÿè´§å¸æŠ•èµ„å¹³å°å®£ç§°æ—¥èµšåƒå…ƒï¼Œå®é™…ä¸ºä¼ é”€ç»„ç»‡",
                "label": "danger",
                "category": "financial", 
                "source": "çœŸå®æ¡ˆä¾‹:plustokenä¼ é”€æ¡ˆ"
            },
            {
                "text": "P2Pç†è´¢å¹³å°å£°ç§°å›½å®¶èƒŒæ™¯ï¼Œå¹´åŒ–20%æ”¶ç›Šï¼Œæœ€ç»ˆçˆ†é›·è·‘è·¯",
                "label": "danger",
                "category": "financial",
                "source": "çœŸå®æ¡ˆä¾‹:å¤šå®¶P2På¹³å°æš´é›·äº‹ä»¶"
            },
            {
                "text": "å…»è€é‡‘èéª—å±€ï¼Œä»¥é«˜é¢å›æŠ¥å¸å¼•è€å¹´äººæŠ•èµ„ï¼Œè¡€æœ¬æ— å½’",
                "label": "danger",
                "category": "financial",
                "source": "çœŸå®æ¡ˆä¾‹:å…»è€è¯ˆéª—ä¸“é¡¹æ•´æ²»æ¡ˆä¾‹"
            },
            {
                "text": "å¤–æ±‡äº¤æ˜“å¹³å°è™šå‡å®£ä¼ ï¼Œæ‰¿è¯º100%ç›ˆåˆ©ï¼Œè¯±å¯¼å¤§é¢æŠ•èµ„",
                "label": "danger",
                "category": "financial",
                "source": "çœŸå®æ¡ˆä¾‹:å¤–æ±‡é»‘å¹³å°è¯ˆéª—"
            },
            {
                "text": "è‚¡ç¥¨æ¨èç¾¤å†…å¹•æ¶ˆæ¯ï¼Œè·Ÿå•å¿…èµšï¼Œå®ä¸ºæ“çºµå¸‚åœº",
                "label": "danger",
                "category": "financial",
                "source": "çœŸå®æ¡ˆä¾‹:èè‚¡è¯ˆéª—å›¢ä¼™"
            },
            {
                "text": "å…è´¹é¢†å–å…»è€è¡¥è´´ï¼Œè¦æ±‚æä¾›é“¶è¡Œå¡ä¿¡æ¯ï¼Œç›—å–è´¦æˆ·èµ„é‡‘",
                "label": "danger",
                "category": "financial",
                "source": "çœŸå®æ¡ˆä¾‹:å…»è€è¡¥è´´ç”µä¿¡è¯ˆéª—"
            },
            {
                "text": "æŠ•èµ„ç†è´¢éœ€è°¨æ…ï¼Œé€‰æ‹©æ­£è§„é‡‘èæœºæ„ï¼Œäº†è§£é£é™©ç­‰çº§",
                "label": "safe",
                "category": "financial",
                "source": "å¤®è¡Œé‡‘èçŸ¥è¯†æ™®åŠ"
            },
            {
                "text": "é“¶è¡Œç†è´¢äº§å“æœ‰é£é™©æç¤ºï¼Œæ”¶ç›Šä¸ä¿è¯ï¼ŒæŠ•èµ„éœ€ç†æ€§",
                "label": "safe",
                "category": "financial",
                "source": "é“¶ä¿ç›‘ä¼šæŠ•èµ„è€…æ•™è‚²"
            },
        ]
        
        # åŒ»ç–—å¥åº·è™šå‡ä¿¡æ¯çœŸå®æ¡ˆä¾‹
        health_misinformation = [
            {
                "text": "æƒå¥ä¿å¥å“å®£ç§°åŒ…æ²»ç™¾ç—…ï¼Œç«ç–—èƒ½æ²»ç™Œç—‡ï¼Œè‡´äººæ­»äº¡",
                "label": "danger",
                "category": "health",
                "source": "çœŸå®æ¡ˆä¾‹:æƒå¥äº‹ä»¶"
            },
            {
                "text": "é¸¿èŒ…è¯é…’è™šå‡å®£ä¼ æ²»ç–—å¤šç§ç–¾ç—…ï¼Œå®é™…è¿è§„æ·»åŠ æˆåˆ†",
                "label": "danger",
                "category": "health",
                "source": "çœŸå®æ¡ˆä¾‹:é¸¿èŒ…è¯é…’äº‹ä»¶"
            },
            {
                "text": "ç½‘ä¼ é£Ÿç‰©ç›¸å…‹è¡¨ç§‘å­¦æ€§å­˜ç–‘ï¼Œä¸“å®¶è¾Ÿè°£æ— ç§‘å­¦ä¾æ®",
                "label": "danger",
                "category": "health",
                "source": "çœŸå®æ¡ˆä¾‹:é£Ÿç‰©ç›¸å…‹è°£è¨€"
            },
            {
                "text": "ç¥åŒ»å¼ æ‚Ÿæœ¬ç»¿è±†æ²»ç™¾ç—…ç†è®ºè¢«è¯ä¼ªï¼Œè¯¯å¯¼å¤§é‡æ‚£è€…",
                "label": "danger",
                "category": "health",
                "source": "çœŸå®æ¡ˆä¾‹:å¼ æ‚Ÿæœ¬äº‹ä»¶"
            },
            {
                "text": "é…¸ç¢±ä½“è´¨ç†è®ºè¢«ç¾å›½æ³•é™¢åˆ¤å®šä¸ºéª—å±€ï¼Œåˆ›å§‹äººç½šæ¬¾1.05äº¿ç¾å…ƒ",
                "label": "danger",
                "category": "health",
                "source": "çœŸå®æ¡ˆä¾‹:é…¸ç¢±ä½“è´¨éª—å±€"
            },
            {
                "text": "å‡è‚¥è¯è™šå‡å®£ä¼ ä¸€å‘¨ç˜¦20æ–¤ï¼Œå®é™…å«è¿ç¦æˆåˆ†å¯¼è‡´å¥åº·æŸå®³",
                "label": "danger",
                "category": "health",
                "source": "çœŸå®æ¡ˆä¾‹:éæ³•å‡è‚¥è¯æ¡ˆä»¶"
            },
            {
                "text": "å¥åº·é¥®é£Ÿéœ€å‡è¡¡è¥å…»ï¼Œéµå¾ªè†³é£ŸæŒ‡å—ï¼Œé€‚é‡è¿åŠ¨",
                "label": "safe",
                "category": "health",
                "source": "å›½å®¶å«å¥å§”å¥åº·ä¸­å›½è¡ŒåŠ¨"
            },
            {
                "text": "å°±åŒ»è¯·é€‰æ‹©æ­£è§„åŒ»é™¢ï¼ŒéµåŒ»å˜±ç”¨è¯ï¼Œå®šæœŸä½“æ£€",
                "label": "safe",
                "category": "health",
                "source": "åŒ»ç–—å«ç”Ÿæœºæ„è§„èŒƒæŒ‡å¯¼"
            },
        ]
        
        # ç§‘æŠ€è°£è¨€çœŸå®æ¡ˆä¾‹
        tech_misinformation = [
            {
                "text": "5GåŸºç«™è¾å°„è‡´ç™Œè°£è¨€ï¼Œä¸“å®¶è¾Ÿè°£ç¬¦åˆå›½é™…æ ‡å‡†",
                "label": "danger",
                "category": "technology",
                "source": "çœŸå®æ¡ˆä¾‹:5Gè¾å°„è°£è¨€"
            },
            {
                "text": "å¾®æ³¢ç‚‰åŠ çƒ­é£Ÿç‰©äº§ç”Ÿè‡´ç™Œç‰©è´¨ï¼Œå®ä¸ºè™šå‡ä¿¡æ¯",
                "label": "danger",
                "category": "technology",
                "source": "çœŸå®æ¡ˆä¾‹:å¾®æ³¢ç‚‰è°£è¨€"
            },
            {
                "text": "æ‰‹æœºç”µæ± çˆ†ç‚¸è°£è¨€å¤¸å¤§å…¶è¯ï¼Œæ­£å¸¸ä½¿ç”¨å®‰å…¨å¯é ",
                "label": "warning",
                "category": "technology",
                "source": "çœŸå®æ¡ˆä¾‹:æ‰‹æœºç”µæ± è°£è¨€"
            },
        ]
        
        # ç¤¾ä¼šçƒ­ç‚¹è°£è¨€
        social_misinformation = [
            {
                "text": "æŸåœ°è‡ªæ¥æ°´æœ‰æ¯’è°£è¨€å¼•å‘æŠ¢è´­ï¼Œå®˜æ–¹è¾Ÿè°£æ°´è´¨å®‰å…¨",
                "label": "danger",
                "category": "social",
                "source": "çœŸå®æ¡ˆä¾‹:è‡ªæ¥æ°´è°£è¨€"
            },
            {
                "text": "é£Ÿç›æŠ¢è´­è°£è¨€é€ æˆææ…Œï¼Œå®é™…ä¾›åº”å……è¶³",
                "label": "danger",
                "category": "social",
                "source": "çœŸå®æ¡ˆä¾‹:æŠ¢ç›é£æ³¢"
            },
            {
                "text": "åœ°éœ‡è°£è¨€å¼•å‘ç¤¾ä¼šææ…Œï¼Œåœ°éœ‡å±€åŠæ—¶è¾Ÿè°£",
                "label": "danger",
                "category": "social",
                "source": "çœŸå®æ¡ˆä¾‹:åœ°éœ‡è°£è¨€"
            },
        ]
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        all_data = (
            financial_scams + 
            health_misinformation + 
            tech_misinformation + 
            social_misinformation
        )
        
        # æ ‡ç­¾æ˜ å°„
        label_map = {'safe': 0, 'warning': 1, 'danger': 2}
        
        # ä¿å­˜æ•°æ®é›†
        output_path = self.base_dir / "real_cases" / "real_case_dataset.json"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        processed_data = []
        for item in all_data:
            processed_data.append({
                "text": item["text"],
                "label": label_map[item["label"]],
                "category": item["category"],
                "source": item["source"],
                "timestamp": "2024-2025"
            })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(processed_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"çœŸå®æ¡ˆä¾‹æ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(processed_data)} æ¡")
        logger.info(f"ä¿å­˜è·¯å¾„: {output_path}")
        
        return len(processed_data)
    
    def expand_existing_datasets(self) -> int:
        """æ‰©å……ç°æœ‰æ•°æ®é›†"""
        logger.info("æ‰©å……ç°æœ‰æ•°æ®é›†...")
        
        total_expanded = 0
        
        # è¯»å–ç°æœ‰MCFENDæ•°æ®å¹¶æ‰©å±•
        mcfend_path = self.base_dir / "mcfend" / "mcfend_data.json"
        if mcfend_path.exists():
            try:
                with open(mcfend_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # å¦‚æœæ•°æ®é‡å°‘ï¼Œç”Ÿæˆæ‰©å±•ç‰ˆæœ¬
                if isinstance(data, list) and len(data) < 100:
                    expanded_data = data.copy()
                    
                    # æ•°æ®å¢å¼ºï¼šæ·»åŠ å˜ä½“
                    for item in data[:20]:  # å¯¹å‰20æ¡è¿›è¡Œæ‰©å±•
                        if isinstance(item, dict) and 'text' in item:
                            # åˆ›å»ºè½»å¾®å˜ä½“
                            expanded_data.append({
                                **item,
                                "text": item["text"] + "ï¼Œè¯·å¤§å®¶æ³¨æ„è¾¨åˆ«ã€‚"
                            })
                    
                    # ä¿å­˜æ‰©å±•ç‰ˆæœ¬
                    expanded_path = self.base_dir / "mcfend" / "mcfend_expanded.json"
                    with open(expanded_path, 'w', encoding='utf-8') as f:
                        json.dump(expanded_data, f, ensure_ascii=False, indent=2)
                    
                    total_expanded += len(expanded_data) - len(data)
                    logger.info(f"MCFENDæ•°æ®é›†æ‰©å±•: +{len(expanded_data) - len(data)} æ¡")
            except Exception as e:
                logger.warning(f"æ‰©å±•MCFENDæ•°æ®å¤±è´¥: {e}")
        
        # ç±»ä¼¼åœ°æ‰©å±•Weiboæ•°æ®
        weibo_path = self.base_dir / "weibo_rumors" / "weibo_data.json"
        if weibo_path.exists():
            try:
                with open(weibo_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                if isinstance(data, list) and len(data) < 100:
                    expanded_data = data.copy()
                    
                    for item in data[:20]:
                        if isinstance(item, dict) and 'text' in item:
                            expanded_data.append({
                                **item,
                                "text": item["text"] + "ã€‚æ­¤æ¶ˆæ¯éœ€æ ¸å®ã€‚"
                            })
                    
                    expanded_path = self.base_dir / "weibo_rumors" / "weibo_expanded.json"
                    with open(expanded_path, 'w', encoding='utf-8') as f:
                        json.dump(expanded_data, f, ensure_ascii=False, indent=2)
                    
                    total_expanded += len(expanded_data) - len(data)
                    logger.info(f"Weiboæ•°æ®é›†æ‰©å±•: +{len(expanded_data) - len(data)} æ¡")
            except Exception as e:
                logger.warning(f"æ‰©å±•Weiboæ•°æ®å¤±è´¥: {e}")
        
        return total_expanded
    
    def download_public_apis_data(self):
        """ä»å…¬å¼€APIä¸‹è½½çœŸå®æ•°æ®"""
        logger.info("å°è¯•ä»å…¬å¼€APIè·å–æ•°æ®...")
        
        # ç¤ºä¾‹ï¼šä»GitHubå…¬å¼€ä»“åº“è·å–æ•°æ®
        public_repos = [
            {
                "name": "Chinese_Rumor_Dataset",
                "url": "https://raw.githubusercontent.com/thunlp/Chinese_Rumor_Dataset/master/rumors_v170613.json",
                "path": "chinese_rumor"
            }
        ]
        
        downloaded_count = 0
        
        for repo in public_repos:
            try:
                logger.info(f"ä¸‹è½½ {repo['name']}...")
                response = requests.get(repo['url'], timeout=30)
                
                if response.status_code == 200:
                    output_dir = self.base_dir / repo['path']
                    output_dir.mkdir(parents=True, exist_ok=True)
                    
                    output_file = output_dir / f"{repo['name']}.json"
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    
                    logger.info(f"æˆåŠŸä¸‹è½½: {output_file}")
                    downloaded_count += 1
                else:
                    logger.warning(f"ä¸‹è½½å¤±è´¥: {repo['name']}, çŠ¶æ€ç : {response.status_code}")
                    
            except Exception as e:
                logger.warning(f"ä¸‹è½½ {repo['name']} å¤±è´¥: {e}")
        
        return downloaded_count
    
    def generate_comprehensive_training_set(self) -> str:
        """ç”Ÿæˆç»¼åˆè®­ç»ƒé›†"""
        logger.info("ç”Ÿæˆç»¼åˆè®­ç»ƒæ•°æ®é›†...")
        
        all_samples = []
        
        # æ”¶é›†æ‰€æœ‰æ•°æ®æº
        data_sources = [
            self.base_dir / "real_cases" / "real_case_dataset.json",
            self.base_dir / "mcfend" / "mcfend_data.json",
            self.base_dir / "mcfend" / "mcfend_expanded.json",
            self.base_dir / "weibo_rumors" / "weibo_data.json",
            self.base_dir / "weibo_rumors" / "weibo_expanded.json",
        ]
        
        for source_path in data_sources:
            if source_path.exists():
                try:
                    with open(source_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    if isinstance(data, list):
                        all_samples.extend(data)
                        logger.info(f"åŠ è½½ {source_path.name}: {len(data)} æ¡")
                except Exception as e:
                    logger.warning(f"åŠ è½½ {source_path} å¤±è´¥: {e}")
        
        # ä¿å­˜ç»¼åˆæ•°æ®é›†
        output_path = self.base_dir / "comprehensive_training_set.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(all_samples, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»¼åˆè®­ç»ƒé›†åˆ›å»ºå®Œæˆ: {len(all_samples)} æ¡æ ·æœ¬")
        logger.info(f"ä¿å­˜è·¯å¾„: {output_path}")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_dataset_report(all_samples, output_path)
        
        return str(output_path)
    
    def _generate_dataset_report(self, samples: List[Dict], output_path: Path):
        """ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š"""
        report = {
            "total_samples": len(samples),
            "label_distribution": {},
            "category_distribution": {},
            "sources": set(),
            "generated_at": json.dumps({"timestamp": "2025-09-20"})
        }
        
        for sample in samples:
            # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
            label = sample.get('label', 'unknown')
            if isinstance(label, int):
                label_name = ['safe', 'warning', 'danger'][label] if label < 3 else 'unknown'
            else:
                label_name = label
            report["label_distribution"][label_name] = report["label_distribution"].get(label_name, 0) + 1
            
            # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
            category = sample.get('category', 'unknown')
            report["category_distribution"][category] = report["category_distribution"].get(category, 0) + 1
            
            # æ”¶é›†æ•°æ®æº
            source = sample.get('source', 'unknown')
            report["sources"].add(source)
        
        report["sources"] = list(report["sources"])
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = output_path.parent / "dataset_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®é›†æŠ¥å‘Š: {report_path}")
        logger.info(f"æ ‡ç­¾åˆ†å¸ƒ: {report['label_distribution']}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹ä¸‹è½½å’Œå¤„ç†çœŸå®æ•°æ®é›†...")
    
    downloader = RealDatasetDownloader()
    
    # 1. åˆ›å»ºåŸºäºçœŸå®æ¡ˆä¾‹çš„æ•°æ®é›†
    real_cases_count = downloader.create_synthetic_realistic_dataset()
    
    # 2. æ‰©å……ç°æœ‰æ•°æ®é›†
    expanded_count = downloader.expand_existing_datasets()
    
    # 3. å°è¯•ä¸‹è½½å…¬å¼€æ•°æ®
    # downloaded_count = downloader.download_public_apis_data()
    
    # 4. ç”Ÿæˆç»¼åˆè®­ç»ƒé›†
    comprehensive_path = downloader.generate_comprehensive_training_set()
    
    print(f"""
ğŸ‰ çœŸå®æ•°æ®é›†å‡†å¤‡å®Œæˆï¼
==========================================

ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:
   çœŸå®æ¡ˆä¾‹: {real_cases_count} æ¡
   æ‰©å±•æ•°æ®: {expanded_count} æ¡
   
ğŸ’¾ è¾“å‡ºæ–‡ä»¶:
   ç»¼åˆè®­ç»ƒé›†: {comprehensive_path}
   
ğŸš€ ä¸‹ä¸€æ­¥:
   è¿è¡Œ python advanced_ai_trainer.py ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒæ¨¡å‹
   
==========================================
    """)

if __name__ == "__main__":
    main()
