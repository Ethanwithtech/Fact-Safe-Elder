#!/usr/bin/env python3
"""
å¢å¼ºçš„AIæ£€æµ‹åç«¯æœåŠ¡
æ”¯æŒè§†é¢‘ä¸Šä¼ ã€å¤šæ¨¡æ€æ£€æµ‹ã€çœŸå®AIæ¨¡å‹è®­ç»ƒ
"""

import os
import sys
import json
import asyncio
import time
import uuid
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path

from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
import uvicorn
from loguru import logger

# AIå’Œæœºå™¨å­¦ä¹ 
try:
    import torch
    import numpy as np
    from transformers import (
        AutoTokenizer, 
        AutoModelForSequenceClassification,
        pipeline,
        Trainer,
        TrainingArguments
    )
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support
    from datasets import Dataset
    import cv2
    TORCH_AVAILABLE = True
except ImportError as e:
    logger.warning(f"AIä¾èµ–åŒ…æœªå®Œå…¨å®‰è£…: {e}")
    TORCH_AVAILABLE = False

# åˆ›å»ºå¿…è¦ç›®å½•
os.makedirs("data/uploads", exist_ok=True)
os.makedirs("data/processed", exist_ok=True)
os.makedirs("models/trained", exist_ok=True)
os.makedirs("logs/training", exist_ok=True)

app = FastAPI(
    title="AIå®ˆæŠ¤ç³»ç»Ÿ - å¢å¼ºç‰ˆåç«¯",
    description="æ”¯æŒè§†é¢‘æ£€æµ‹ã€æ¨¡å‹è®­ç»ƒã€å¤šæ¨¡æ€åˆ†æçš„AIæ£€æµ‹æœåŠ¡",
    version="2.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
training_tasks = {}
detection_stats = {
    "total_detections": 0,
    "danger_count": 0,
    "warning_count": 0,
    "safe_count": 0,
    "accuracy": 0.0,
    "models_loaded": 0
}

class AIDetectionEngine:
    """AIæ£€æµ‹å¼•æ“"""
    
    def __init__(self):
        self.models = {}
        self.tokenizers = {}
        self.device = torch.device("cuda" if torch.cuda.is_available() and TORCH_AVAILABLE else "cpu")
        self.training_data = []
        self.model_metrics = {}
        
        logger.info(f"AIæ£€æµ‹å¼•æ“åˆå§‹åŒ–ï¼Œè®¾å¤‡: {self.device}")
        
    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            if TORCH_AVAILABLE:
                await self._load_models()
            else:
                logger.warning("PyTorchä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            
            # åŠ è½½è®­ç»ƒæ•°æ®
            await self._load_training_data()
            
        except Exception as e:
            logger.error(f"AIå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def _load_models(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            # åŠ è½½ä¸­æ–‡BERTæ¨¡å‹ç”¨äºæ–‡æœ¬åˆ†ç±»
            logger.info("æ­£åœ¨åŠ è½½BERTæ¨¡å‹...")
            self.tokenizers['bert'] = AutoTokenizer.from_pretrained(
                'hfl/chinese-bert-wwm-ext',
                cache_dir='./models/cache'
            )
            
            # åˆ›å»ºåˆ†ç±»æ¨¡å‹
            self.models['bert'] = AutoModelForSequenceClassification.from_pretrained(
                'hfl/chinese-bert-wwm-ext',
                num_labels=3,  # safe, warning, danger
                cache_dir='./models/cache'
            ).to(self.device)
            
            detection_stats["models_loaded"] += 1
            logger.info("BERTæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # å°è¯•åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹æƒé‡
            trained_model_path = "models/trained/bert_fake_news_detector.pt"
            if os.path.exists(trained_model_path):
                checkpoint = torch.load(trained_model_path, map_location=self.device)
                self.models['bert'].load_state_dict(checkpoint['model_state_dict'])
                self.model_metrics = checkpoint.get('metrics', {})
                logger.info(f"å·²åŠ è½½è®­ç»ƒæƒé‡ï¼Œå‡†ç¡®ç‡: {self.model_metrics.get('accuracy', 0):.2%}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    async def _load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        try:
            # ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
            self.training_data = [
                {"text": "æœˆå…¥ä¸‰ä¸‡çš„ç†è´¢ç§˜è¯€ï¼Œä¿è¯æ”¶ç›Šæ— é£é™©ï¼", "label": 2},  # danger
                {"text": "ç¥–ä¼ ç§˜æ–¹åŒ…æ²»ç™¾ç—…ï¼Œä¸‰å¤©è§æ•ˆï¼", "label": 2},  # danger
                {"text": "é™æ—¶æŠ¢è´­ï¼ŒåŸä»·999ç°åœ¨99ï¼", "label": 1},  # warning
                {"text": "ä»Šå¤©æ•™å¤§å®¶åšçº¢çƒ§è‚‰çš„å®¶å¸¸åšæ³•", "label": 0},  # safe
                {"text": "æŠ•èµ„éœ€è°¨æ…ï¼Œè¯·ä»”ç»†é˜…è¯»äº§å“è¯´æ˜ä¹¦", "label": 0},  # safe
                {"text": "ç¥å¥‡ä¿å¥å“ï¼ŒåŒ»é™¢ä¸å‘Šè¯‰ä½ çš„ç§˜å¯†", "label": 2},  # danger
                {"text": "æ­£è§„é“¶è¡Œç†è´¢äº§å“ï¼Œå¹´åŒ–æ”¶ç›Š3.5%", "label": 0},  # safe
                {"text": "å¿«é€Ÿèµšé’±æ–¹æ³•ï¼Œæ—¥èµšåƒå…ƒä¸æ˜¯æ¢¦", "label": 2},  # danger
                {"text": "å¥åº·é¥®é£Ÿå°è´´å£«ï¼Œè¥å…»å‡è¡¡å¾ˆé‡è¦", "label": 0},  # safe
                {"text": "å†…å¹•æ¶ˆæ¯ï¼Œè‚¡ç¥¨æ¨èï¼Œç¨³èµšä¸èµ”", "label": 2},  # danger
            ]
            
            # å°è¯•åŠ è½½çœŸå®æ•°æ®é›†
            data_files = [
                "data/raw/mcfend/mcfend_data.json",
                "data/raw/weibo_rumors/weibo_data.json",
                "data/raw/chinese_rumor/rumors_v170613.json"
            ]
            
            for data_file in data_files:
                if os.path.exists(data_file):
                    with open(data_file, 'r', encoding='utf-8') as f:
                        real_data = json.load(f)
                        if isinstance(real_data, list):
                            self.training_data.extend(real_data[:1000])  # é™åˆ¶æ•°æ®é‡
                            logger.info(f"åŠ è½½çœŸå®æ•°æ®: {data_file}, {len(real_data)} æ¡")
            
            logger.info(f"è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆï¼Œæ€»è®¡: {len(self.training_data)} æ¡")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    async def detect_text(self, text: str) -> Dict:
        """æ–‡æœ¬æ£€æµ‹"""
        try:
            if 'bert' in self.models and TORCH_AVAILABLE:
                return await self._detect_with_bert(text)
            else:
                return self._detect_with_rules(text)
                
        except Exception as e:
            logger.error(f"æ–‡æœ¬æ£€æµ‹å¤±è´¥: {e}")
            return self._get_fallback_result(text)
    
    async def _detect_with_bert(self, text: str) -> Dict:
        """ä½¿ç”¨BERTæ¨¡å‹æ£€æµ‹"""
        try:
            tokenizer = self.tokenizers['bert']
            model = self.models['bert']
            
            # æ–‡æœ¬ç¼–ç 
            inputs = tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            ).to(self.device)
            
            # æ¨¡å‹æ¨ç†
            model.eval()
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=-1)
            
            # è·å–é¢„æµ‹ç»“æœ
            predicted_class = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_class].item()
            
            # æ˜ å°„åˆ°é£é™©ç­‰çº§
            risk_levels = ['safe', 'warning', 'danger']
            level = risk_levels[predicted_class]
            
            # ç”Ÿæˆè¯¦ç»†åˆ†æ
            reasons = self._analyze_text_risks(text, predicted_class)
            suggestions = self._generate_suggestions(level)
            
            return {
                'success': True,
                'level': level,
                'score': float(confidence),
                'confidence': float(confidence),
                'message': f'BERTæ¨¡å‹æ£€æµ‹å®Œæˆï¼Œé£é™©ç­‰çº§ï¼š{level}',
                'reasons': reasons,
                'suggestions': suggestions,
                'model': 'BERT-Chinese',
                'probabilities': {
                    'safe': float(probabilities[0][0]),
                    'warning': float(probabilities[0][1]),
                    'danger': float(probabilities[0][2])
                },
                'detection_id': f"bert_{int(time.time())}_{hash(text) % 10000}",
                'timestamp': time.time()
            }
            
        except Exception as e:
            logger.error(f"BERTæ£€æµ‹å¤±è´¥: {e}")
            return self._get_fallback_result(text)
    
    def _detect_with_rules(self, text: str) -> Dict:
        """åŸºäºè§„åˆ™çš„æ£€æµ‹"""
        # å¼ºåŒ–çš„å…³é”®è¯æ£€æµ‹
        danger_keywords = [
            "ä¿è¯æ”¶ç›Š", "æœˆå…¥ä¸‡å…ƒ", "åŒ…æ²»ç™¾ç—…", "ç¥–ä¼ ç§˜æ–¹", "æ— é£é™©æŠ•èµ„", 
            "æœˆå…¥", "ä¸‡å…ƒ", "åŒ…æ²»", "ç§˜æ–¹", "ä¿è¯", "æ— é£é™©", "ç†è´¢ç§˜è¯€",
            "ç¨³èµšä¸èµ”", "é«˜æ”¶ç›Š", "å†…å¹•æ¶ˆæ¯", "è‚¡ç¥¨æ¨è", "è™šæ‹Ÿè´§å¸",
            "ä¼ é”€", "å¾®å•†ä»£ç†", "æ‹‰äººå¤´", "æ— æŠµæŠ¼è´·æ¬¾", "ç§’æ‰¹",
            "ç¥å¥‡ç–—æ•ˆ", "ä¸€æ¬¡æ ¹æ²»", "æ°¸ä¸å¤å‘", "è¯åˆ°ç—…é™¤", "æ—¥èµšåƒå…ƒ"
        ]
        
        warning_keywords = [
            "æŠ•èµ„", "ç†è´¢", "ä¿å¥å“", "åæ–¹", "å¾®ä¿¡", "è”ç³»", "æ”¶ç›Š", "èµšé’±",
            "é™æ—¶", "ä¼˜æƒ ", "æŠ¢è´­", "ç‰¹ä»·", "æ¸…ä»“", "ä¿ƒé”€", "ä»£ç†", "åŠ ç›Ÿ",
            "å¿«é€Ÿ", "è½»æ¾", "ç®€å•", "ç«‹å³", "é©¬ä¸Š", "ç´§æ€¥"
        ]
        
        risk_level = "safe"
        risk_score = 0.1
        reasons = []
        
        # æ£€æµ‹å±é™©å…³é”®è¯
        found_danger = [kw for kw in danger_keywords if kw in text]
        if found_danger:
            risk_level = "danger"
            risk_score = min(0.95, 0.7 + len(found_danger) * 0.05)
            reasons.extend([f"å‘ç°é«˜å±å…³é”®è¯: '{kw}'" for kw in found_danger[:3]])
        else:
            # æ£€æµ‹è­¦å‘Šå…³é”®è¯
            found_warning = [kw for kw in warning_keywords if kw in text]
            if found_warning:
                risk_level = "warning"
                risk_score = min(0.8, 0.4 + len(found_warning) * 0.05)
                reasons.extend([f"å‘ç°å¯ç–‘å…³é”®è¯: '{kw}'" for kw in found_warning[:3]])
        
        # æ£€æµ‹è”ç³»æ–¹å¼
        import re
        if re.search(r'å¾®ä¿¡|qq|ç”µè¯|æ‰‹æœº|è”ç³»|\d{11}', text):
            if risk_level != "safe":
                risk_score = min(0.98, risk_score + 0.1)
                reasons.append("å«æœ‰è”ç³»æ–¹å¼ä¸”å­˜åœ¨å…¶ä»–é£é™©å› ç´ ")
        
        # æ£€æµ‹ç´§æ€¥æ€§è¯­è¨€
        urgency_words = ['èµ¶ç´§', 'ç«‹å³', 'é©¬ä¸Š', 'å¿«é€Ÿ', 'ç´§æ€¥', 'é™æ—¶', 'æˆªæ­¢', 'æœ€å']
        urgency_count = sum(1 for word in urgency_words if word in text)
        if urgency_count >= 2:
            if risk_level == "safe":
                risk_level = "warning"
                risk_score = max(risk_score, 0.6)
            reasons.append("å†…å®¹ä½¿ç”¨å¤§é‡ç´§æ€¥æ€§è¯­è¨€ï¼Œå¯èƒ½æ˜¯è¯±å¯¼æ‰‹æ®µ")
        
        if not reasons:
            reasons.append("æœªå‘ç°æ˜æ˜¾é£é™©å› ç´ ")
        
        suggestions = self._generate_suggestions(risk_level)
        
        return {
            'success': True,
            'level': risk_level,
            'score': risk_score,
            'confidence': 0.8,
            'message': f'è§„åˆ™æ£€æµ‹å®Œæˆï¼Œé£é™©ç­‰çº§ï¼š{risk_level}',
            'reasons': reasons,
            'suggestions': suggestions,
            'model': 'Rule-Based',
            'keywords_found': found_danger + found_warning if 'found_danger' in locals() and 'found_warning' in locals() else [],
            'detection_id': f"rule_{int(time.time())}_{hash(text) % 10000}",
            'timestamp': time.time()
        }
    
    def _analyze_text_risks(self, text: str, predicted_class: int) -> List[str]:
        """åˆ†ææ–‡æœ¬é£é™©å› ç´ """
        reasons = []
        
        if predicted_class == 2:  # danger
            reasons.extend([
                "AIæ¨¡å‹æ£€æµ‹åˆ°é«˜é£é™©å†…å®¹ç‰¹å¾",
                "æ–‡æœ¬è¯­ä¹‰åˆ†ææ˜¾ç¤ºå­˜åœ¨æ¬ºè¯ˆé£é™©",
                "å†…å®¹æ¨¡å¼åŒ¹é…åˆ°å·²çŸ¥è¯ˆéª—ç±»å‹"
            ])
        elif predicted_class == 1:  # warning
            reasons.extend([
                "AIæ¨¡å‹æ£€æµ‹åˆ°å¯ç–‘å†…å®¹ç‰¹å¾",
                "æ–‡æœ¬å­˜åœ¨éœ€è¦æ³¨æ„çš„é£é™©å› ç´ "
            ])
        else:  # safe
            reasons.append("AIæ¨¡å‹æœªæ£€æµ‹åˆ°æ˜æ˜¾é£é™©")
        
        return reasons
    
    def _generate_suggestions(self, risk_level: str) -> List[str]:
        """ç”Ÿæˆå®‰å…¨å»ºè®®"""
        if risk_level == "danger":
            return [
                "å»ºè®®ç«‹å³åœæ­¢è§‚çœ‹ï¼Œè°¨é˜²è¯ˆéª—",
                "ä¸è¦è½»æ˜“ç›¸ä¿¡é«˜æ”¶ç›Šä½é£é™©çš„æŠ•èµ„é¡¹ç›®",
                "ä¸è¦å‘é™Œç”Ÿäººé€éœ²ä¸ªäººä¿¡æ¯æˆ–è½¬è´¦",
                "å¦‚æœ‰ç–‘é—®ï¼Œè¯·å’¨è¯¢å®¶äººæˆ–ä¸“ä¸šäººå£«"
            ]
        elif risk_level == "warning":
            return [
                "è¯·è°¨æ…å¯¹å¾…æ­¤å†…å®¹",
                "è´­ä¹°å‰è¯·ä»”ç»†æ ¸å®ä¿¡æ¯çœŸå®æ€§",
                "å»ºè®®å’¨è¯¢ä¸“ä¸šäººå£«æ„è§",
                "ä¸è¦å†²åŠ¨æ¶ˆè´¹æˆ–æŠ•èµ„"
            ]
        else:
            return [
                "å†…å®¹ç›¸å¯¹å®‰å…¨",
                "ç»§ç»­ä¿æŒè­¦æƒ•æ„è¯†",
                "é‡åˆ°å¯ç–‘å†…å®¹åŠæ—¶æ±‚åŠ©"
            ]
    
    def _get_fallback_result(self, text: str) -> Dict:
        """é™çº§ç»“æœ"""
        return {
            'success': True,
            'level': 'warning',
            'score': 0.5,
            'confidence': 0.5,
            'message': 'AIæ¨¡å‹æš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥',
            'reasons': ['ç³»ç»Ÿæ£€æµ‹å¼‚å¸¸ï¼Œå»ºè®®äººå·¥å¤æ ¸'],
            'suggestions': ['è¯·è°¨æ…å¯¹å¾…æ­¤å†…å®¹', 'å¦‚æœ‰ç–‘é—®è¯·å’¨è¯¢ä¸“ä¸šäººå£«'],
            'model': 'Fallback',
            'detection_id': f"fallback_{int(time.time())}",
            'timestamp': time.time()
        }
    
    async def detect_video(self, video_path: str, text_content: str = "") -> Dict:
        """è§†é¢‘æ£€æµ‹"""
        try:
            # æå–è§†é¢‘ç‰¹å¾
            video_features = await self._extract_video_features(video_path)
            
            # æ–‡æœ¬æ£€æµ‹
            text_result = await self.detect_text(text_content) if text_content else None
            
            # ç»¼åˆåˆ†æ
            final_result = self._combine_multimodal_results(video_features, text_result)
            
            return final_result
            
        except Exception as e:
            logger.error(f"è§†é¢‘æ£€æµ‹å¤±è´¥: {e}")
            return self._get_fallback_result(f"è§†é¢‘å†…å®¹: {text_content}")
    
    async def _extract_video_features(self, video_path: str) -> Dict:
        """æå–è§†é¢‘ç‰¹å¾"""
        try:
            if not os.path.exists(video_path):
                raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            # ä½¿ç”¨OpenCVæå–è§†é¢‘å¸§
            cap = cv2.VideoCapture(video_path)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = frame_count / fps if fps > 0 else 0
            
            # æå–å…³é”®å¸§
            frames = []
            for i in range(0, frame_count, max(1, frame_count // 10)):  # æå–10å¸§
                cap.set(cv2.CAP_PROP_POS_FRAMES, i)
                ret, frame = cap.read()
                if ret:
                    frames.append(frame)
            
            cap.release()
            
            # åˆ†æè§†é¢‘ç‰¹å¾
            features = {
                'duration': duration,
                'frame_count': frame_count,
                'fps': fps,
                'key_frames': len(frames),
                'visual_risk': self._analyze_visual_content(frames),
                'technical_risk': self._analyze_technical_features(duration, fps)
            }
            
            return features
            
        except Exception as e:
            logger.error(f"è§†é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
            return {'visual_risk': 0.3, 'technical_risk': 0.2}
    
    def _analyze_visual_content(self, frames: List) -> float:
        """åˆ†æè§†è§‰å†…å®¹"""
        # ç®€å•çš„è§†è§‰åˆ†æï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„CVæ¨¡å‹ï¼‰
        if not frames:
            return 0.3
        
        # æ£€æµ‹äº®åº¦å˜åŒ–ã€é¢œè‰²åˆ†å¸ƒç­‰
        risk_score = 0.0
        
        for frame in frames[:5]:  # åˆ†æå‰5å¸§
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # è®¡ç®—äº®åº¦
            brightness = np.mean(gray)
            
            # å¼‚å¸¸äº®åº¦å¯èƒ½è¡¨ç¤ºä½è´¨é‡å†…å®¹
            if brightness < 50 or brightness > 200:
                risk_score += 0.1
        
        return min(risk_score, 0.8)
    
    def _analyze_technical_features(self, duration: float, fps: float) -> float:
        """åˆ†ææŠ€æœ¯ç‰¹å¾"""
        risk_score = 0.0
        
        # å¼‚å¸¸çŸ­çš„è§†é¢‘å¯èƒ½æ˜¯åƒåœ¾å†…å®¹
        if duration < 5:
            risk_score += 0.2
        
        # å¼‚å¸¸ä½çš„å¸§ç‡å¯èƒ½è¡¨ç¤ºä½è´¨é‡
        if fps < 15:
            risk_score += 0.1
        
        return min(risk_score, 0.5)
    
    def _combine_multimodal_results(self, video_features: Dict, text_result: Dict) -> Dict:
        """ç»¼åˆå¤šæ¨¡æ€ç»“æœ"""
        # è§†è§‰é£é™©æƒé‡
        visual_risk = video_features.get('visual_risk', 0.3)
        technical_risk = video_features.get('technical_risk', 0.2)
        
        # æ–‡æœ¬é£é™©æƒé‡
        if text_result:
            text_risk = text_result['score']
            text_level = text_result['level']
        else:
            text_risk = 0.3
            text_level = 'safe'
        
        # åŠ æƒè®¡ç®—æœ€ç»ˆé£é™©
        final_risk = text_risk * 0.7 + visual_risk * 0.2 + technical_risk * 0.1
        
        # ç¡®å®šæœ€ç»ˆç­‰çº§
        if final_risk > 0.7 or text_level == 'danger':
            final_level = 'danger'
        elif final_risk > 0.4 or text_level == 'warning':
            final_level = 'warning'
        else:
            final_level = 'safe'
        
        return {
            'success': True,
            'level': final_level,
            'score': final_risk,
            'confidence': 0.85,
            'message': f'å¤šæ¨¡æ€æ£€æµ‹å®Œæˆï¼Œé£é™©ç­‰çº§ï¼š{final_level}',
            'reasons': [
                f"æ–‡æœ¬é£é™©: {text_risk:.2f}",
                f"è§†è§‰é£é™©: {visual_risk:.2f}",
                f"æŠ€æœ¯é£é™©: {technical_risk:.2f}"
            ],
            'suggestions': self._generate_suggestions(final_level),
            'model': 'Multimodal-AI',
            'multimodal_analysis': {
                'text_risk': text_risk,
                'visual_risk': visual_risk,
                'technical_risk': technical_risk,
                'video_features': video_features
            },
            'detection_id': f"video_{int(time.time())}",
            'timestamp': time.time()
        }
    
    async def train_model(self, task_id: str, config: Dict) -> Dict:
        """è®­ç»ƒæ¨¡å‹"""
        try:
            if not TORCH_AVAILABLE:
                raise Exception("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
            
            logger.info(f"å¼€å§‹è®­ç»ƒä»»åŠ¡: {task_id}")
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            train_data = self._prepare_training_data()
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = Dataset.from_dict({
                'text': [item['text'] for item in train_data],
                'labels': [item['label'] for item in train_data]
            })
            
            # æ•°æ®é¢„å¤„ç†
            def tokenize_function(examples):
                return self.tokenizers['bert'](
                    examples['text'],
                    padding=True,
                    truncation=True,
                    max_length=512
                )
            
            tokenized_dataset = train_dataset.map(tokenize_function, batched=True)
            
            # è®­ç»ƒé…ç½®
            training_args = TrainingArguments(
                output_dir=f'./logs/training/{task_id}',
                num_train_epochs=config.get('epochs', 3),
                per_device_train_batch_size=config.get('batch_size', 8),
                learning_rate=config.get('learning_rate', 5e-5),
                logging_steps=10,
                save_steps=100,
                evaluation_strategy="steps",
                eval_steps=50,
                load_best_model_at_end=True,
                metric_for_best_model="accuracy"
            )
            
            # è¯„ä¼°å‡½æ•°
            def compute_metrics(eval_pred):
                predictions, labels = eval_pred
                predictions = np.argmax(predictions, axis=1)
                precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
                accuracy = accuracy_score(labels, predictions)
                return {
                    'accuracy': accuracy,
                    'f1': f1,
                    'precision': precision,
                    'recall': recall
                }
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = Trainer(
                model=self.models['bert'],
                args=training_args,
                train_dataset=tokenized_dataset,
                eval_dataset=tokenized_dataset,  # å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨éªŒè¯é›†
                compute_metrics=compute_metrics
            )
            
            # å¼€å§‹è®­ç»ƒ
            training_tasks[task_id] = {
                'status': 'training',
                'progress': 0,
                'start_time': time.time(),
                'metrics': {}
            }
            
            # è®­ç»ƒæ¨¡å‹
            train_result = trainer.train()
            
            # è¯„ä¼°æ¨¡å‹
            eval_result = trainer.evaluate()
            
            # ä¿å­˜æ¨¡å‹
            model_save_path = f"models/trained/bert_fake_news_detector_{task_id}.pt"
            torch.save({
                'model_state_dict': self.models['bert'].state_dict(),
                'metrics': eval_result,
                'config': config,
                'training_result': train_result
            }, model_save_path)
            
            # æ›´æ–°å…¨å±€æŒ‡æ ‡
            self.model_metrics = eval_result
            detection_stats["accuracy"] = eval_result.get('eval_accuracy', 0)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            training_tasks[task_id] = {
                'status': 'completed',
                'progress': 100,
                'start_time': training_tasks[task_id]['start_time'],
                'end_time': time.time(),
                'metrics': eval_result,
                'model_path': model_save_path
            }
            
            logger.info(f"è®­ç»ƒå®Œæˆ: {task_id}, å‡†ç¡®ç‡: {eval_result.get('eval_accuracy', 0):.2%}")
            
            return {
                'success': True,
                'task_id': task_id,
                'metrics': eval_result,
                'model_path': model_save_path
            }
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            training_tasks[task_id] = {
                'status': 'failed',
                'progress': 0,
                'error': str(e),
                'start_time': training_tasks.get(task_id, {}).get('start_time', time.time())
            }
            raise
    
    def _prepare_training_data(self) -> List[Dict]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # æ‰©å……è®­ç»ƒæ•°æ®
        extended_data = self.training_data.copy()
        
        # æ·»åŠ æ›´å¤šæ ·æœ¬
        additional_samples = [
            {"text": "æŠ•èµ„ç†è´¢æœ‰é£é™©ï¼Œè¯·è°¨æ…é€‰æ‹©", "label": 0},
            {"text": "æ­£è§„åŒ»é™¢æ²»ç–—ï¼ŒéµåŒ»å˜±ç”¨è¯", "label": 0},
            {"text": "å¤©ä¸‹æ²¡æœ‰å…è´¹çš„åˆé¤ï¼Œå°å¿ƒé™·é˜±", "label": 0},
            {"text": "ä¸€å¤œæš´å¯Œçš„æœºä¼šæ¥äº†ï¼Œé”™è¿‡åæ‚”ç»ˆç”Ÿ", "label": 2},
            {"text": "ç‰¹æ•ˆè¯åŒ…æ²»ç™¾ç—…ï¼Œæ— æ•ˆé€€æ¬¾", "label": 2},
            {"text": "å†…å¹•æ¶ˆæ¯ï¼Œè‚¡ç¥¨å¿…æ¶¨ï¼Œè·Ÿæˆ‘ä¹°", "label": 2},
            {"text": "é™æ—¶ä¼˜æƒ ï¼Œæ•°é‡æœ‰é™ï¼Œå…ˆåˆ°å…ˆå¾—", "label": 1},
            {"text": "å¥åº·ç”Ÿæ´»æ–¹å¼ï¼Œå‡è¡¡é¥®é£Ÿè¿åŠ¨", "label": 0},
            {"text": "å­¦ä¹ ç†è´¢çŸ¥è¯†ï¼Œç†æ€§æŠ•èµ„", "label": 0},
        ]
        
        extended_data.extend(additional_samples)
        return extended_data

# åˆ›å»ºå…¨å±€AIå¼•æ“
ai_engine = AIDetectionEngine()

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    await ai_engine.initialize()

# APIç«¯ç‚¹
@app.get("/")
async def root():
    return {
        "message": "AIå®ˆæŠ¤ç³»ç»Ÿå¢å¼ºç‰ˆåç«¯",
        "version": "2.0.0",
        "status": "è¿è¡Œä¸­",
        "models_loaded": detection_stats["models_loaded"],
        "accuracy": f"{detection_stats['accuracy']:.2%}"
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "message": "æœåŠ¡è¿è¡Œæ­£å¸¸",
        "training_data": len(ai_engine.training_data),
        "models_loaded": detection_stats["models_loaded"],
        "accuracy": detection_stats["accuracy"]
    }

@app.post("/detect")
async def detect_text(request: dict):
    """æ–‡æœ¬æ£€æµ‹API"""
    try:
        text = request.get("text", "")
        if not text:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        # æ‰§è¡Œæ£€æµ‹
        result = await ai_engine.detect_text(text)
        
        # æ›´æ–°ç»Ÿè®¡
        detection_stats["total_detections"] += 1
        if result['level'] == 'danger':
            detection_stats["danger_count"] += 1
        elif result['level'] == 'warning':
            detection_stats["warning_count"] += 1
        else:
            detection_stats["safe_count"] += 1
        
        return result
        
    except Exception as e:
        logger.error(f"æ–‡æœ¬æ£€æµ‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ£€æµ‹å¤±è´¥: {str(e)}")

@app.post("/detect/video")
async def detect_video(
    file: UploadFile = File(...),
    text_content: str = Form(default="")
):
    """è§†é¢‘æ£€æµ‹API"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„è§†é¢‘
        video_id = str(uuid.uuid4())
        video_path = f"data/uploads/{video_id}_{file.filename}"
        
        with open(video_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # æ‰§è¡Œæ£€æµ‹
        result = await ai_engine.detect_video(video_path, text_content)
        
        # æ›´æ–°ç»Ÿè®¡
        detection_stats["total_detections"] += 1
        if result['level'] == 'danger':
            detection_stats["danger_count"] += 1
        elif result['level'] == 'warning':
            detection_stats["warning_count"] += 1
        else:
            detection_stats["safe_count"] += 1
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        # os.remove(video_path)
        
        return result
        
    except Exception as e:
        logger.error(f"è§†é¢‘æ£€æµ‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è§†é¢‘æ£€æµ‹å¤±è´¥: {str(e)}")

@app.post("/train")
async def start_training(
    background_tasks: BackgroundTasks,
    config: dict
):
    """å¯åŠ¨æ¨¡å‹è®­ç»ƒ"""
    try:
        task_id = str(uuid.uuid4())
        
        # æ·»åŠ åå°è®­ç»ƒä»»åŠ¡
        background_tasks.add_task(ai_engine.train_model, task_id, config)
        
        return {
            "success": True,
            "message": "è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨",
            "task_id": task_id,
            "config": config
        }
        
    except Exception as e:
        logger.error(f"å¯åŠ¨è®­ç»ƒå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨è®­ç»ƒå¤±è´¥: {str(e)}")

@app.get("/train/status/{task_id}")
async def get_training_status(task_id: str):
    """è·å–è®­ç»ƒçŠ¶æ€"""
    if task_id not in training_tasks:
        raise HTTPException(status_code=404, detail="è®­ç»ƒä»»åŠ¡ä¸å­˜åœ¨")
    
    return {
        "success": True,
        "task_id": task_id,
        **training_tasks[task_id]
    }

@app.get("/stats")
async def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡"""
    return {
        "success": True,
        "stats": detection_stats,
        "model_metrics": ai_engine.model_metrics,
        "training_data_size": len(ai_engine.training_data)
    }

@app.get("/models/status")
async def get_models_status():
    """è·å–æ¨¡å‹çŠ¶æ€"""
    return {
        "success": True,
        "models": {
            "bert": {
                "loaded": 'bert' in ai_engine.models,
                "device": str(ai_engine.device),
                "metrics": ai_engine.model_metrics
            }
        },
        "torch_available": TORCH_AVAILABLE,
        "device": str(ai_engine.device)
    }

if __name__ == "__main__":
    print("""
ğŸ›¡ï¸ ========================================
   AIå®ˆæŠ¤ç³»ç»Ÿ - å¢å¼ºç‰ˆåç«¯å¯åŠ¨
   æ”¯æŒè§†é¢‘æ£€æµ‹ + æ¨¡å‹è®­ç»ƒ + å¤šæ¨¡æ€åˆ†æ
======================================== ğŸ›¡ï¸
    """)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8008,
        log_level="info"
    )
