#!/usr/bin/env python3
"""
çœŸå®AIæ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨çœŸå®æ•°æ®é›†è®­ç»ƒBERTã€ChatGLMç­‰æ¨¡å‹
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    import torch
    from torch.utils.data import DataLoader, Dataset
    from transformers import (
        AutoTokenizer, 
        AutoModelForSequenceClassification,
        TrainingArguments,
        Trainer,
        DataCollatorWithPadding
    )
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
    from datasets import Dataset as HFDataset
    TORCH_AVAILABLE = True
    logger.info("AIè®­ç»ƒä¾èµ–åŒ…åŠ è½½æˆåŠŸ")
except ImportError as e:
    logger.error(f"AIè®­ç»ƒä¾èµ–åŒ…æœªå®‰è£…: {e}")
    TORCH_AVAILABLE = False

class FakeNewsDataset:
    """è™šå‡ä¿¡æ¯æ•°æ®é›†ç±»"""
    
    def __init__(self):
        self.data = []
        self.labels = []
        self.label_map = {'safe': 0, 'warning': 1, 'danger': 2}
        self.reverse_label_map = {0: 'safe', 1: 'warning', 2: 'danger'}
    
    def load_data(self) -> Tuple[List[str], List[int]]:
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        logger.info("å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®...")
        
        # 1. åŠ è½½å†…ç½®ç¤ºä¾‹æ•°æ®
        self._load_builtin_data()
        
        # 2. å°è¯•åŠ è½½çœŸå®æ•°æ®é›†
        self._load_real_datasets()
        
        # 3. æ•°æ®å¢å¼º
        self._augment_data()
        
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆï¼Œæ€»è®¡: {len(self.data)} æ¡æ ·æœ¬")
        logger.info(f"æ ‡ç­¾åˆ†å¸ƒ: {self._get_label_distribution()}")
        
        return self.data, self.labels
    
    def _load_builtin_data(self):
        """åŠ è½½å†…ç½®æ•°æ®"""
        builtin_data = [
            # é‡‘èè¯ˆéª—ç±» (danger = 2)
            ("æœˆå…¥ä¸‰ä¸‡çš„ç†è´¢ç§˜è¯€ï¼Œä¿è¯æ”¶ç›Šæ— é£é™©ï¼è·Ÿç€è€å¸ˆæ“ä½œï¼Œç¨³èµšä¸èµ”ï¼", 2),
            ("æŠ•èµ„ç†è´¢æ–°æœºä¼šï¼Œä¸€å¤œæš´å¯Œä¸æ˜¯æ¢¦ï¼å†…å¹•æ¶ˆæ¯ï¼Œå¿…æ¶¨è‚¡ç¥¨ï¼", 2),
            ("æ— æŠµæŠ¼è´·æ¬¾ï¼Œç§’æ‰¹ç§’æ”¾ï¼Œåˆ©æ¯è¶…ä½ï¼Œä¸çœ‹å¾ä¿¡ï¼", 2),
            ("è™šæ‹Ÿè´§å¸æŠ•èµ„ï¼Œæ—¥èµšåƒå…ƒï¼Œé”™è¿‡åæ‚”ç»ˆç”Ÿï¼", 2),
            ("ä¿è¯æ”¶ç›Š20%ï¼Œæ— é£é™©æŠ•èµ„ï¼Œé“¶è¡Œå†…éƒ¨äº§å“ï¼", 2),
            ("è·Ÿç€ä¸“å®¶ç‚’è‚¡ï¼ŒåŒ…èµšä¸èµ”ï¼Œæœˆæ”¶ç›Šç¿»å€ï¼", 2),
            ("P2Pç†è´¢å¹³å°ï¼Œå¹´åŒ–æ”¶ç›Š30%ï¼Œå›½å®¶èƒŒæ™¯ï¼", 2),
            ("å¤–æ±‡æŠ•èµ„ï¼Œå°æŠ•å…¥å¤§å›æŠ¥ï¼Œä¸“ä¸šæŒ‡å¯¼ï¼", 2),
            ("æœŸè´§äº¤æ˜“å†…å¹•ï¼Œè·Ÿå•å¿…èµšï¼Œèµ„é‡‘å®‰å…¨ï¼", 2),
            ("æ•°å­—è´§å¸æŒ–çŸ¿ï¼Œèººç€èµšé’±ï¼Œä¸€æœ¬ä¸‡åˆ©ï¼", 2),
            
            # åŒ»ç–—è™šå‡ä¿¡æ¯ç±» (danger = 2)
            ("ç¥–ä¼ ç§˜æ–¹åŒ…æ²»ç™¾ç—…ï¼Œä¸‰å¤©è§æ•ˆï¼ŒåŒ»é™¢ä¸å‘Šè¯‰ä½ çš„ç§˜å¯†ï¼", 2),
            ("ç¥å¥‡ä¿å¥å“ï¼Œä¸€æ¬¡æ ¹æ²»ï¼Œæ°¸ä¸å¤å‘ï¼Œè¯åˆ°ç—…é™¤ï¼", 2),
            ("ç™Œç—‡ç‰¹æ•ˆè¯ï¼Œæ— å‰¯ä½œç”¨ï¼Œæ²»æ„ˆç‡100%ï¼", 2),
            ("å‡è‚¥ç¥å™¨ï¼Œä¸€å‘¨ç˜¦20æ–¤ï¼Œä¸åå¼¹ä¸èŠ‚é£Ÿï¼", 2),
            ("å£®é˜³ç¥è¯ï¼Œä¸€ç²’è§æ•ˆï¼Œé‡æŒ¯é›„é£ï¼", 2),
            ("ç³–å°¿ç—…æ ¹æ²»æ–¹æ³•ï¼Œå‘Šåˆ«èƒ°å²›ç´ ï¼Œå½»åº•æ²»æ„ˆï¼", 2),
            ("é«˜è¡€å‹ç‰¹æ•ˆè¯ï¼Œä¸€ä¸ªç–—ç¨‹æ ¹æ²»ï¼Œä¸ç”¨ç»ˆèº«æœè¯ï¼", 2),
            ("ç™½å‘å˜é»‘å‘ï¼Œå¤©ç„¶æ¤ç‰©é…æ–¹ï¼Œä¸€ä¸ªæœˆè§æ•ˆï¼", 2),
            ("è¿‘è§†çœ¼æ²»ç–—ä»ªï¼Œä¸æ‰‹æœ¯ä¸åƒè¯ï¼Œæ¢å¤è§†åŠ›ï¼", 2),
            ("ç—”ç–®æ ¹æ²»è†ï¼Œä¸‰å¤©è§æ•ˆï¼Œå½»åº•æ ¹æ²»ä¸å¤å‘ï¼", 2),
            
            # å¯ç–‘å†…å®¹ç±» (warning = 1)
            ("é™æ—¶æŠ¢è´­ï¼ŒåŸä»·999ç°åœ¨99ï¼Œæ•°é‡æœ‰é™ï¼", 1),
            ("æŠ•èµ„ç†è´¢æœ‰é£é™©ï¼Œè¯·è°¨æ…é€‰æ‹©ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šäººå£«", 1),
            ("ä¿å¥å“è¾…åŠ©è°ƒç†ï¼Œé…åˆåŒ»ç”Ÿæ²»ç–—æ•ˆæœæ›´ä½³", 1),
            ("ç½‘è´­ä¿ƒé”€æ´»åŠ¨ï¼Œå“è´¨ä¿è¯ï¼Œä¸ƒå¤©æ— ç†ç”±é€€è´§", 1),
            ("å­¦ä¹ æŠ•èµ„çŸ¥è¯†ï¼Œç†æ€§æŠ•èµ„ï¼Œåˆ†æ•£é£é™©", 1),
            ("å¥åº·ç”Ÿæ´»æ–¹å¼ï¼Œå‡è¡¡é¥®é£Ÿï¼Œé€‚é‡è¿åŠ¨", 1),
            ("ç¾å®¹æŠ¤è‚¤äº§å“ï¼Œå› äººè€Œå¼‚ï¼Œå»ºè®®å…ˆè¯•ç”¨", 1),
            ("å‡è‚¥äº§å“è¾…åŠ©ï¼Œéœ€é…åˆè¿åŠ¨å’Œé¥®é£Ÿæ§åˆ¶", 1),
            ("ç†è´¢è§„åˆ’å»ºè®®ï¼Œæ ¹æ®ä¸ªäººæƒ…å†µåˆ¶å®šæ–¹æ¡ˆ", 1),
            ("ä¿é™©äº§å“ä»‹ç»ï¼Œè¯¦ç»†äº†è§£æ¡æ¬¾å†è´­ä¹°", 1),
            
            # å®‰å…¨å†…å®¹ç±» (safe = 0)
            ("ä»Šå¤©æ•™å¤§å®¶åšçº¢çƒ§è‚‰çš„å®¶å¸¸åšæ³•ï¼Œç®€å•æ˜“å­¦", 0),
            ("å¤©æ°”é¢„æŠ¥ï¼šæ˜å¤©å¤šäº‘è½¬æ™´ï¼Œæ°”æ¸©15-25åº¦", 0),
            ("æ–°é—»èµ„è®¯ï¼šæœ¬å¸‚å°†æ–°å»ºä¸‰æ‰€å­¦æ ¡", 0),
            ("å¥åº·æé†’ï¼šå¤šå–æ°´ï¼Œæ³¨æ„ä¼‘æ¯ï¼Œé¢„é˜²æ„Ÿå†’", 0),
            ("äº¤é€šå‡ºè¡Œï¼šåœ°é“2å·çº¿ä»Šæ—¥æ­£å¸¸è¿è¥", 0),
            ("å­¦ä¹ åˆ†äº«ï¼šå¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡çš„å‡ ä¸ªæ–¹æ³•", 0),
            ("ç”Ÿæ´»å°è´´å£«ï¼šå¦‚ä½•æ­£ç¡®ä¿å­˜è”¬èœæ°´æœ", 0),
            ("è¿åŠ¨å¥èº«ï¼šé€‚åˆåˆå­¦è€…çš„ç‘œä¼½åŠ¨ä½œ", 0),
            ("æ–‡åŒ–å¨±ä¹ï¼šæœ¬å‘¨æœ«åšç‰©é¦†æœ‰ç‰¹å±•æ´»åŠ¨", 0),
            ("ç§‘æ™®çŸ¥è¯†ï¼šä¸ºä»€ä¹ˆä¼šæœ‰å››å­£å˜åŒ–", 0),
            ("æ­£è§„é“¶è¡Œç†è´¢äº§å“ï¼Œå¹´åŒ–æ”¶ç›Š3.5%ï¼Œé£é™©éœ€è°¨æ…", 0),
            ("åŒ»é™¢æ­£è§„æ²»ç–—ï¼ŒéµåŒ»å˜±ç”¨è¯ï¼Œå®šæœŸå¤æŸ¥", 0),
            ("å¥åº·é¥®é£Ÿï¼Œè¥å…»å‡è¡¡ï¼Œé€‚é‡è¿åŠ¨å¾ˆé‡è¦", 0),
            ("å­¦ä¹ ç†è´¢çŸ¥è¯†ï¼Œäº†è§£é£é™©ï¼Œç†æ€§æŠ•èµ„", 0),
            ("è´­ä¹°ä¿é™©å‰ï¼Œä»”ç»†é˜…è¯»æ¡æ¬¾ï¼Œäº†è§£ä¿éšœèŒƒå›´", 0),
        ]
        
        for text, label in builtin_data:
            self.data.append(text)
            self.labels.append(label)
        
        logger.info(f"åŠ è½½å†…ç½®æ•°æ®: {len(builtin_data)} æ¡")
    
    def _load_real_datasets(self):
        """åŠ è½½çœŸå®æ•°æ®é›†"""
        dataset_files = [
            ("data/raw/mcfend/mcfend_data.json", self._parse_mcfend),
            ("data/raw/weibo_rumors/weibo_data.json", self._parse_weibo),
            ("data/raw/chinese_rumor/rumors_v170613.json", self._parse_chinese_rumor),
            ("data/raw/liar/train.tsv", self._parse_liar)
        ]
        
        for file_path, parser in dataset_files:
            if os.path.exists(file_path):
                try:
                    count = parser(file_path)
                    logger.info(f"åŠ è½½çœŸå®æ•°æ®é›†: {file_path}, {count} æ¡")
                except Exception as e:
                    logger.warning(f"åŠ è½½æ•°æ®é›†å¤±è´¥ {file_path}: {e}")
    
    def _parse_mcfend(self, file_path: str) -> int:
        """è§£æMCFENDæ•°æ®é›†"""
        count = 0
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            for item in data[:500]:  # é™åˆ¶æ•°é‡
                if isinstance(item, dict) and 'text' in item and 'label' in item:
                    text = item['text']
                    label = item['label']
                    
                    # æ˜ å°„æ ‡ç­¾
                    if label in ['fake', 'rumor', 'false']:
                        mapped_label = 2  # danger
                    elif label in ['real', 'true']:
                        mapped_label = 0  # safe
                    else:
                        mapped_label = 1  # warning
                    
                    self.data.append(text)
                    self.labels.append(mapped_label)
                    count += 1
        
        return count
    
    def _parse_weibo(self, file_path: str) -> int:
        """è§£æå¾®åšæ•°æ®é›†"""
        count = 0
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            for item in data[:300]:  # é™åˆ¶æ•°é‡
                if isinstance(item, dict) and 'text' in item:
                    text = item['text']
                    # ç®€å•çš„æ ‡ç­¾åˆ¤æ–­
                    if any(word in text for word in ['è™šå‡', 'è°£è¨€', 'å‡çš„', 'ä¸å®']):
                        label = 2  # danger
                    else:
                        label = 0  # safe
                    
                    self.data.append(text)
                    self.labels.append(label)
                    count += 1
        
        return count
    
    def _parse_chinese_rumor(self, file_path: str) -> int:
        """è§£æä¸­æ–‡è°£è¨€æ•°æ®é›†"""
        count = 0
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            for item in data[:200]:  # é™åˆ¶æ•°é‡
                if isinstance(item, dict) and 'text' in item and 'label' in item:
                    text = item['text']
                    label = 2 if item['label'] == 'rumor' else 0
                    
                    self.data.append(text)
                    self.labels.append(label)
                    count += 1
        
        return count
    
    def _parse_liar(self, file_path: str) -> int:
        """è§£æLIARæ•°æ®é›†"""
        count = 0
        try:
            df = pd.read_csv(file_path, sep='\t', header=None)
            
            for _, row in df.head(100).iterrows():  # é™åˆ¶æ•°é‡
                if len(row) > 2:
                    text = str(row[2])  # æ–‡æœ¬å†…å®¹
                    # ç®€å•æ˜ å°„
                    label = 1  # warning (è‹±æ–‡æ•°æ®é›†ï¼Œæ ‡è®°ä¸ºè­¦å‘Š)
                    
                    self.data.append(text)
                    self.labels.append(label)
                    count += 1
        except Exception as e:
            logger.warning(f"è§£æLIARæ•°æ®é›†å¤±è´¥: {e}")
        
        return count
    
    def _augment_data(self):
        """æ•°æ®å¢å¼º"""
        # ç®€å•çš„æ•°æ®å¢å¼ºï¼šä¸ºå°‘æ•°ç±»åˆ«å¢åŠ æ ·æœ¬
        label_counts = {0: 0, 1: 0, 2: 0}
        for label in self.labels:
            label_counts[label] += 1
        
        max_count = max(label_counts.values())
        
        # ä¸ºæ¯ä¸ªç±»åˆ«è¡¥å……æ•°æ®
        original_data = list(zip(self.data, self.labels))
        
        for target_label in [0, 1, 2]:
            current_count = label_counts[target_label]
            if current_count < max_count // 2:  # å¦‚æœæ ·æœ¬æ•°å°‘äºæœ€å¤§å€¼çš„ä¸€åŠ
                # å¤åˆ¶ä¸€äº›æ ·æœ¬
                samples_to_add = min(max_count // 2 - current_count, current_count)
                label_samples = [(text, label) for text, label in original_data if label == target_label]
                
                for i in range(samples_to_add):
                    text, label = label_samples[i % len(label_samples)]
                    self.data.append(text)
                    self.labels.append(label)
        
        logger.info(f"æ•°æ®å¢å¼ºå®Œæˆï¼Œæ–°å¢æ ·æœ¬: {len(self.data) - len(original_data)}")
    
    def _get_label_distribution(self) -> Dict[str, int]:
        """è·å–æ ‡ç­¾åˆ†å¸ƒ"""
        distribution = {}
        for label in self.labels:
            label_name = self.reverse_label_map[label]
            distribution[label_name] = distribution.get(label_name, 0) + 1
        return distribution

class AIModelTrainer:
    """AIæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_name: str = "hfl/chinese-bert-wwm-ext"):
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = None
        self.model = None
        self.training_results = {}
        
        logger.info(f"åˆå§‹åŒ–AIæ¨¡å‹è®­ç»ƒå™¨ï¼Œè®¾å¤‡: {self.device}")
    
    def prepare_model(self):
        """å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info(f"åŠ è½½æ¨¡å‹: {self.model_name}")
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                cache_dir='./models/cache'
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                num_labels=3,  # safe, warning, danger
                cache_dir='./models/cache'
            ).to(self.device)
            
            logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def prepare_data(self, texts: List[str], labels: List[int], test_size: float = 0.2):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # åˆ†å‰²æ•°æ®
        train_texts, test_texts, train_labels, test_labels = train_test_split(
            texts, labels, test_size=test_size, random_state=42, stratify=labels
        )
        
        # åˆ›å»ºæ•°æ®é›†
        def tokenize_function(examples):
            return self.tokenizer(
                examples['text'],
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
        
        train_dataset = HFDataset.from_dict({
            'text': train_texts,
            'labels': train_labels
        })
        
        test_dataset = HFDataset.from_dict({
            'text': test_texts,
            'labels': test_labels
        })
        
        train_dataset = train_dataset.map(tokenize_function, batched=True)
        test_dataset = test_dataset.map(tokenize_function, batched=True)
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
        
        return train_dataset, test_dataset
    
    def train(self, train_dataset, test_dataset, 
              epochs: int = 3, 
              batch_size: int = 8,
              learning_rate: float = 5e-5):
        """è®­ç»ƒæ¨¡å‹"""
        logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=f'./logs/training/{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            learning_rate=learning_rate,
            weight_decay=0.01,
            logging_dir='./logs/training',
            logging_steps=10,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="accuracy",
            greater_is_better=True,
            report_to=None,  # ä¸ä½¿ç”¨wandbç­‰
            save_total_limit=2
        )
        
        # è¯„ä¼°å‡½æ•°
        def compute_metrics(eval_pred):
            predictions, labels = eval_pred
            predictions = np.argmax(predictions, axis=1)
            
            accuracy = accuracy_score(labels, predictions)
            precision, recall, f1, _ = precision_recall_fscore_support(
                labels, predictions, average='weighted'
            )
            
            return {
                'accuracy': accuracy,
                'f1': f1,
                'precision': precision,
                'recall': recall
            }
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,
            compute_metrics=compute_metrics,
            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer)
        )
        
        # å¼€å§‹è®­ç»ƒ
        train_result = trainer.train()
        
        # è¯„ä¼°æ¨¡å‹
        eval_result = trainer.evaluate()
        
        # ä¿å­˜ç»“æœ
        self.training_results = {
            'train_loss': train_result.training_loss,
            'eval_accuracy': eval_result['eval_accuracy'],
            'eval_f1': eval_result['eval_f1'],
            'eval_precision': eval_result['eval_precision'],
            'eval_recall': eval_result['eval_recall'],
            'training_time': train_result.metrics['train_runtime'],
            'model_name': self.model_name,
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate
        }
        
        logger.info(f"è®­ç»ƒå®Œæˆï¼å‡†ç¡®ç‡: {eval_result['eval_accuracy']:.4f}")
        
        return self.training_results
    
    def save_model(self, save_path: str):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        logger.info(f"ä¿å­˜æ¨¡å‹åˆ°: {save_path}")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹æƒé‡å’Œè®­ç»ƒç»“æœ
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'tokenizer': self.tokenizer,
            'training_results': self.training_results,
            'model_name': self.model_name
        }, save_path)
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        report_path = save_path.replace('.pt', '_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_results, f, ensure_ascii=False, indent=2)
        
        logger.info("æ¨¡å‹ä¿å­˜å®Œæˆ")
    
    def test_model(self, test_texts: List[str], test_labels: List[int]):
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
        logger.info("æµ‹è¯•æ¨¡å‹æ€§èƒ½...")
        
        self.model.eval()
        predictions = []
        
        with torch.no_grad():
            for text in test_texts:
                inputs = self.tokenizer(
                    text,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                ).to(self.device)
                
                outputs = self.model(**inputs)
                pred = torch.argmax(outputs.logits, dim=-1).item()
                predictions.append(pred)
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        accuracy = accuracy_score(test_labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            test_labels, predictions, average='weighted'
        )
        
        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        target_names = ['safe', 'warning', 'danger']
        report = classification_report(
            test_labels, predictions, 
            target_names=target_names,
            output_dict=True
        )
        
        logger.info(f"æµ‹è¯•ç»“æœ - å‡†ç¡®ç‡: {accuracy:.4f}, F1: {f1:.4f}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'classification_report': report
        }

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    if not TORCH_AVAILABLE:
        logger.error("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒ")
        return
    
    logger.info("å¼€å§‹AIæ¨¡å‹è®­ç»ƒæµç¨‹")
    
    try:
        # 1. åŠ è½½æ•°æ®
        dataset = FakeNewsDataset()
        texts, labels = dataset.load_data()
        
        if len(texts) < 50:
            logger.warning("è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œå»ºè®®æ·»åŠ æ›´å¤šæ•°æ®")
        
        # 2. åˆå§‹åŒ–è®­ç»ƒå™¨
        trainer = AIModelTrainer()
        trainer.prepare_model()
        
        # 3. å‡†å¤‡æ•°æ®
        train_dataset, test_dataset = trainer.prepare_data(texts, labels)
        
        # 4. è®­ç»ƒæ¨¡å‹
        results = trainer.train(
            train_dataset=train_dataset,
            test_dataset=test_dataset,
            epochs=5,
            batch_size=8,
            learning_rate=5e-5
        )
        
        # 5. ä¿å­˜æ¨¡å‹
        save_path = f"models/trained/bert_fake_news_detector_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt"
        trainer.save_model(save_path)
        
        # 6. æµ‹è¯•æ¨¡å‹
        test_texts = texts[-20:]  # ä½¿ç”¨æœ€å20ä¸ªæ ·æœ¬æµ‹è¯•
        test_labels = labels[-20:]
        test_results = trainer.test_model(test_texts, test_labels)
        
        # 7. æ‰“å°ç»“æœ
        print(f"""
ğŸ‰ AIæ¨¡å‹è®­ç»ƒå®Œæˆï¼
==========================================

ğŸ“Š è®­ç»ƒç»“æœ:
   å‡†ç¡®ç‡: {results['eval_accuracy']:.2%}
   F1åˆ†æ•°: {results['eval_f1']:.4f}
   ç²¾ç¡®ç‡: {results['eval_precision']:.4f}
   å¬å›ç‡: {results['eval_recall']:.4f}
   è®­ç»ƒæ—¶é—´: {results['training_time']:.1f}ç§’

ğŸ“ˆ æµ‹è¯•ç»“æœ:
   æµ‹è¯•å‡†ç¡®ç‡: {test_results['accuracy']:.2%}
   æµ‹è¯•F1åˆ†æ•°: {test_results['f1']:.4f}

ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {save_path}

ğŸ¯ æ•°æ®ç»Ÿè®¡:
   æ€»æ ·æœ¬æ•°: {len(texts)}
   è®­ç»ƒé›†: {len(train_dataset)}
   æµ‹è¯•é›†: {len(test_dataset)}
   æ ‡ç­¾åˆ†å¸ƒ: {dataset._get_label_distribution()}

==========================================
        """)
        
        # åˆ›å»ºæœ€æ–°æ¨¡å‹çš„è½¯é“¾æ¥
        latest_path = "models/trained/bert_fake_news_detector.pt"
        if os.path.exists(save_path):
            if os.path.exists(latest_path):
                os.remove(latest_path)
            
            # åœ¨Windowsä¸Šä½¿ç”¨copyè€Œä¸æ˜¯symlink
            import shutil
            shutil.copy2(save_path, latest_path)
            logger.info(f"åˆ›å»ºæœ€æ–°æ¨¡å‹é“¾æ¥: {latest_path}")
        
        logger.info("AIæ¨¡å‹è®­ç»ƒæµç¨‹å®Œæˆ")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

if __name__ == "__main__":
    main()
