# ğŸš€ AIå®ˆæŠ¤ç³»ç»Ÿ - å®Œæ•´éƒ¨ç½²ä¸ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
2. [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
3. [å¿«é€Ÿéƒ¨ç½²](#å¿«é€Ÿéƒ¨ç½²)
4. [AIæ¨¡å‹é…ç½®](#aiæ¨¡å‹é…ç½®)
5. [ä½¿ç”¨è¯´æ˜](#ä½¿ç”¨è¯´æ˜)
6. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ¯ ç³»ç»Ÿæ¦‚è¿°

**AIå®ˆæŠ¤** æ˜¯ä¸€ä¸ªä¸“ä¸ºè€å¹´äººè®¾è®¡çš„çŸ­è§†é¢‘è™šå‡ä¿¡æ¯å®æ—¶æ£€æµ‹ç³»ç»Ÿï¼Œé›†æˆäº†æœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ã€‚

### æ ¸å¿ƒåŠŸèƒ½

#### ğŸ¤– AIæ£€æµ‹èƒ½åŠ›
- **ChatGLM-6B**: ä¸­æ–‡è¯­è¨€ç†è§£å’Œå¯¹è¯ç”Ÿæˆ
- **BERT-Chinese**: æ–‡æœ¬åˆ†ç±»å’Œç‰¹å¾æå–
- **LLaMA-7B**: å¤šè¯­è¨€ç†è§£å’Œæ¨ç†
- **å¤šæ¨¡æ€èåˆ**: æ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘ç»¼åˆåˆ†æ

#### ğŸ“± ç”¨æˆ·ç•Œé¢
- **æ‰‹æœºè§†é¢‘æ¨¡æ‹Ÿå™¨**: 1:1è¿˜åŸæŠ–éŸ³ç•Œé¢
- **æ™ºèƒ½æ£€æµ‹æµ®çª—**: å®æ—¶é£é™©æç¤º
- **é€‚è€åŒ–è®¾è®¡**: å¤§å­—ä½“ã€é«˜å¯¹æ¯”åº¦ã€è¯­éŸ³æç¤º

#### ğŸ“ æ¨¡å‹è®­ç»ƒå¹³å°
- **æ•°æ®é›†ç®¡ç†**: MCFENDã€å¾®åšç­‰å¤šæºæ•°æ®
- **åœ¨çº¿è®­ç»ƒ**: LoRAå¾®è°ƒã€åˆ†å¸ƒå¼è®­ç»ƒ
- **æ€§èƒ½ç›‘æ§**: å®æ—¶è®­ç»ƒè¿›åº¦ã€æŒ‡æ ‡å¯è§†åŒ–

---

## ğŸ’» ç¯å¢ƒè¦æ±‚

### æœ€ä½é…ç½®
```yaml
CPU: 8æ ¸å¿ƒä»¥ä¸Š
å†…å­˜: 16GB RAM
æ˜¾å¡: NVIDIA GTX 1060 (6GB) æˆ–æ›´é«˜
å­˜å‚¨: 100GBå¯ç”¨ç©ºé—´
ç³»ç»Ÿ: Windows 10/11, Ubuntu 20.04+, macOS 11+
```

### æ¨èé…ç½®
```yaml
CPU: 16æ ¸å¿ƒ
å†…å­˜: 32GB RAM
æ˜¾å¡: NVIDIA RTX 3080 (10GB) æˆ– A100
å­˜å‚¨: 500GB SSD
ç½‘ç»œ: 100Mbpsç¨³å®šè¿æ¥
```

### è½¯ä»¶ä¾èµ–
```yaml
Docker: 20.10+
Docker Compose: 2.0+
Node.js: 18+
Python: 3.9+
CUDA: 11.8+ (å¦‚ä½¿ç”¨GPU)
Git: 2.0+
```

---

## âš¡ å¿«é€Ÿéƒ¨ç½²

### 1ï¸âƒ£ è·å–ä»£ç 
```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-org/fact-safe-elder.git
cd fact-safe-elder

# åˆ‡æ¢åˆ°AIç‰ˆæœ¬åˆ†æ”¯
git checkout ai-integration
```

### 2ï¸âƒ£ é…ç½®ç¯å¢ƒå˜é‡
```bash
# å¤åˆ¶ç¯å¢ƒé…ç½®æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘é…ç½®æ–‡ä»¶
nano .env
```

é…ç½®ç¤ºä¾‹ï¼š
```env
# APIé…ç½®
API_HOST=0.0.0.0
API_PORT=8000
FRONTEND_PORT=3000

# AIæ¨¡å‹é…ç½®
USE_GPU=true
MODEL_CACHE_DIR=./models
DEVICE=cuda  # æˆ– cpu

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://user:pass@localhost/elderguard
REDIS_URL=redis://localhost:6379

# å®‰å…¨é…ç½®
SECRET_KEY=your-secret-key-here
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
```

### 3ï¸âƒ£ ä¸€é”®éƒ¨ç½²

#### Dockeræ–¹å¼ï¼ˆæ¨èï¼‰
```bash
# æ„å»ºå¹¶å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up --build -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
```

#### æœ¬åœ°å¼€å‘æ–¹å¼
```bash
# å®‰è£…å‰ç«¯ä¾èµ–
cd frontend
npm install

# å®‰è£…åç«¯ä¾èµ–
cd ../backend
pip install -r requirements.txt

# ä¸‹è½½AIæ¨¡å‹
python scripts/download_models.py

# å¯åŠ¨æœåŠ¡
# ç»ˆç«¯1 - å¯åŠ¨åç«¯
python -m uvicorn app.main:app --reload

# ç»ˆç«¯2 - å¯åŠ¨å‰ç«¯
cd frontend && npm start
```

### 4ï¸âƒ£ éªŒè¯éƒ¨ç½²
```bash
# æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
curl http://localhost:8000/api/health

# è®¿é—®å‰ç«¯ç•Œé¢
open http://localhost:3000

# æµ‹è¯•AIæ¥å£
curl -X POST http://localhost:8000/api/ai/status
```

---

## ğŸ§  AIæ¨¡å‹é…ç½®

### ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹

#### è‡ªåŠ¨ä¸‹è½½
```bash
python scripts/download_models.py --all
```

#### æ‰‹åŠ¨ä¸‹è½½
```bash
# ChatGLM-6B
git lfs clone https://huggingface.co/THUDM/chatglm-6b

# Chinese-BERT
git lfs clone https://huggingface.co/hfl/chinese-bert-wwm-ext

# LLaMA-7B-Chinese
git lfs clone https://huggingface.co/ziqingyang/chinese-llama-7b
```

### æ¨¡å‹å¾®è°ƒ

#### å‡†å¤‡æ•°æ®é›†
```bash
# ä¸‹è½½MCFENDæ•°æ®é›†
python scripts/download_dataset.py --name mcfend

# é¢„å¤„ç†æ•°æ®
python scripts/preprocess_data.py \
  --dataset mcfend \
  --output ./data/processed/
```

#### å¯åŠ¨è®­ç»ƒ
```bash
# ä½¿ç”¨LoRAå¾®è°ƒChatGLM
python train.py \
  --model_type chatglm \
  --dataset mcfend_v1 \
  --epochs 10 \
  --batch_size 8 \
  --learning_rate 5e-5 \
  --use_lora \
  --lora_rank 8
```

#### ç›‘æ§è®­ç»ƒ
è®¿é—® http://localhost:3000 -> æ¨¡å‹è®­ç»ƒ -> æŸ¥çœ‹å®æ—¶è¿›åº¦

---

## ğŸ“± ä½¿ç”¨è¯´æ˜

### åŸºç¡€æ“ä½œ

#### 1. æ‰‹æœºè§†é¢‘æ¨¡æ‹Ÿ
1. æ‰“å¼€ç³»ç»Ÿé¦–é¡µ
2. ç‚¹å‡»"ğŸ“± æ‰‹æœºæ¨¡æ‹Ÿ"æ ‡ç­¾
3. ä¸Šä¸‹æ»‘åŠ¨åˆ‡æ¢è§†é¢‘
4. è§‚å¯Ÿå³ä¾§AIæ£€æµ‹ç»“æœ

#### 2. é£é™©ç­‰çº§è¯´æ˜
- ğŸŸ¢ **ç»¿è‰²ï¼ˆå®‰å…¨ï¼‰**: å†…å®¹å¯ä¿¡ï¼Œå¯ä»¥æ­£å¸¸è§‚çœ‹
- ğŸŸ¡ **é»„è‰²ï¼ˆè­¦å‘Šï¼‰**: å­˜åœ¨é£é™©ï¼Œéœ€è°¨æ…å¯¹å¾…
- ğŸ”´ **çº¢è‰²ï¼ˆå±é™©ï¼‰**: é«˜é£é™©å†…å®¹ï¼Œå»ºè®®ç«‹å³åœæ­¢

#### 3. å®¶äººé€šçŸ¥è®¾ç½®
1. ç‚¹å‡»"âš™ï¸ è®¾ç½®"
2. è¾“å…¥å®¶äººè”ç³»æ–¹å¼
3. è®¾ç½®é€šçŸ¥é˜ˆå€¼
4. ä¿å­˜é…ç½®

### é«˜çº§åŠŸèƒ½

#### æ¨¡å‹è®­ç»ƒ
1. è¿›å…¥"ğŸ“ æ¨¡å‹è®­ç»ƒ"é¡µé¢
2. ç‚¹å‡»"å¼€å§‹æ–°è®­ç»ƒ"
3. é€‰æ‹©æ¨¡å‹å’Œæ•°æ®é›†
4. é…ç½®è®­ç»ƒå‚æ•°
5. ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"
6. å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦

#### æ•°æ®é›†ç®¡ç†
1. è¿›å…¥"ğŸ’¾ æ•°æ®é›†"æ ‡ç­¾
2. ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®é›†
3. æ‰§è¡Œæ•°æ®é¢„å¤„ç†
4. åº”ç”¨æ•°æ®å¢å¼º

#### æ¨¡å‹éƒ¨ç½²
1. è®­ç»ƒå®Œæˆåç‚¹å‡»"éƒ¨ç½²"
2. é€‰æ‹©éƒ¨ç½²ç¯å¢ƒ
3. é…ç½®æ¨ç†å‚æ•°
4. ä¸€é”®éƒ¨ç½²åˆ°ç”Ÿäº§

---

## ğŸ”§ é«˜çº§é…ç½®

### GPUåŠ é€Ÿè®¾ç½®

#### NVIDIA GPU
```bash
# å®‰è£…CUDA
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
sudo sh cuda_11.8.0_520.61.05_linux.run

# é…ç½®ç¯å¢ƒå˜é‡
export CUDA_HOME=/usr/local/cuda-11.8
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# éªŒè¯å®‰è£…
nvidia-smi
nvcc --version
```

#### AMD GPUï¼ˆROCmï¼‰
```bash
# å®‰è£…ROCm
wget https://repo.radeon.com/amdgpu-install/latest/ubuntu/focal/amdgpu-install.deb
sudo apt install ./amdgpu-install.deb
sudo amdgpu-install --usecase=rocm

# å®‰è£…PyTorch for ROCm
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6
```

### æ€§èƒ½ä¼˜åŒ–

#### æ¨¡å‹é‡åŒ–
```python
# INT8é‡åŒ–åŠ é€Ÿæ¨ç†
python scripts/quantize_model.py \
  --model chatglm \
  --quantization int8 \
  --output ./models/chatglm_int8
```

#### æ‰¹å¤„ç†ä¼˜åŒ–
```yaml
# config/inference.yaml
batch_size: 16
max_sequence_length: 512
num_workers: 4
use_mixed_precision: true
```

#### ç¼“å­˜é…ç½®
```python
# é…ç½®Redisç¼“å­˜
CACHE_CONFIG = {
    'host': 'localhost',
    'port': 6379,
    'db': 0,
    'ttl': 3600,  # 1å°æ—¶
    'max_connections': 50
}
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# é”™è¯¯: CUDA out of memory
è§£å†³æ–¹æ¡ˆ:
1. å‡å°batch_size
2. ä½¿ç”¨æ¨¡å‹é‡åŒ–
3. å¯ç”¨gradient checkpointing
4. ä½¿ç”¨CPUæ¨ç†æ¨¡å¼
```

### Q2: è®­ç»ƒé€Ÿåº¦æ…¢
```bash
è§£å†³æ–¹æ¡ˆ:
1. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
2. ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
3. è°ƒæ•´å­¦ä¹ ç‡è°ƒåº¦å™¨
4. ä¼˜åŒ–æ•°æ®åŠ è½½pipeline
```

### Q3: æ£€æµ‹å‡†ç¡®ç‡ä½
```bash
è§£å†³æ–¹æ¡ˆ:
1. å¢åŠ è®­ç»ƒæ•°æ®
2. è°ƒæ•´æ¨¡å‹è¶…å‚æ•°
3. ä½¿ç”¨æ•°æ®å¢å¼º
4. é›†æˆå¤šä¸ªæ¨¡å‹
```

### Q4: Dockerå®¹å™¨æ— æ³•å¯åŠ¨
```bash
# æ£€æŸ¥DockerçŠ¶æ€
sudo systemctl status docker

# æ¸…ç†æ—§å®¹å™¨
docker system prune -a

# é‡æ–°æ„å»º
docker-compose build --no-cache
docker-compose up -d
```

### Q5: å‰ç«¯é¡µé¢åŠ è½½æ…¢
```bash
è§£å†³æ–¹æ¡ˆ:
1. å¯ç”¨ç”Ÿäº§æ¨¡å¼æ„å»º
npm run build

2. é…ç½®Nginxç¼“å­˜
3. ä½¿ç”¨CDNåŠ é€Ÿ
4. å¼€å¯gzipå‹ç¼©
```

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### è·å–å¸®åŠ©
- ğŸ“§ é‚®ç®±: support@elderguard.ai
- ğŸ’¬ å¾®ä¿¡ç¾¤: æ‰«ç åŠ å…¥æŠ€æœ¯äº¤æµç¾¤
- ğŸ“– æ–‡æ¡£: https://docs.elderguard.ai
- ğŸ› æäº¤é—®é¢˜: https://github.com/your-org/fact-safe-elder/issues

### ç¤¾åŒºèµ„æº
- [æ¨¡å‹ä¸‹è½½é•œåƒç«™](https://mirror.elderguard.ai)
- [æ•°æ®é›†å…±äº«å¹³å°](https://data.elderguard.ai)
- [åœ¨çº¿Demoæ¼”ç¤º](https://demo.elderguard.ai)
- [è§†é¢‘æ•™ç¨‹](https://video.elderguard.ai)

### è´¡çŒ®æŒ‡å—
æ¬¢è¿è´¡çŒ®ä»£ç ã€æ–‡æ¡£ã€æ•°æ®é›†ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md)

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®å’Œç ”ç©¶æœºæ„ï¼š
- THUDM (ChatGLM)
- Hugging Face (Transformers)
- Meta AI (LLaMA)
- é¦™æ¸¯æµ¸ä¼šå¤§å­¦ (MCFENDæ•°æ®é›†)
- æ‰€æœ‰è´¡çŒ®è€…å’Œæµ‹è¯•ç”¨æˆ·

---

**è®©AIå®ˆæŠ¤æ¯ä¸€ä½é•¿è¾ˆçš„ç½‘ç»œå®‰å…¨ï¼** ğŸ›¡ï¸ğŸ‘´ğŸ‘µ
