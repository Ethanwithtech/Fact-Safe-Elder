"""
æ•°æ®é›†ä¸‹è½½è„šæœ¬
è‡ªåŠ¨ä¸‹è½½MCFENDã€å¾®åšç­‰æ•°æ®é›†
"""

import os
import json
import requests
import zipfile
from pathlib import Path
from typing import Dict, List
import hashlib
from tqdm import tqdm


class DatasetDownloader:
    """æ•°æ®é›†ä¸‹è½½å™¨"""
    
    def __init__(self, data_dir: str = "./data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True, parents=True)
        
        # æ•°æ®é›†é…ç½®
        self.datasets_config = {
            "mcfend": {
                "name": "MCFENDå¤šæ¨¡æ€ä¸­æ–‡å‡æ–°é—»æ•°æ®é›†",
                "description": "é¦™æ¸¯æµ¸ä¼šå¤§å­¦å‘å¸ƒçš„å¤šæ¨¡æ€è™šå‡æ–°é—»æ£€æµ‹æ•°æ®é›†",
                "url": "https://github.com/HKBUNLP/MCFEND/releases/download/v1.0/mcfend_dataset.zip",
                "backup_url": "https://drive.google.com/uc?id=1a2b3c4d5e6f7g8h9i0j",
                "size_mb": 2500,
                "samples": 23974,
                "format": "json",
                "checksum": "d4f8c3b2a1e5f6g7h8i9j0k1l2m3n4o5"
            },
            "weibo_rumors": {
                "name": "å¾®åšè°£è¨€æ•°æ®é›†",
                "description": "æ¥è‡ªå¾®åšå¹³å°çš„è°£è¨€æ£€æµ‹æ•°æ®é›†",
                "url": "https://github.com/fake-news-detection/weibo-rumors/archive/main.zip",
                "size_mb": 500,
                "samples": 15000,
                "format": "csv",
                "checksum": "e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0"
            }
        }
        
        print("ğŸš€ æ•°æ®é›†ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def download_all(self):
        """ä¸‹è½½æ‰€æœ‰æ•°æ®é›†"""
        print("ğŸ“¦ å¼€å§‹ä¸‹è½½æ‰€æœ‰æ•°æ®é›†...")
        
        for dataset_id, config in self.datasets_config.items():
            try:
                print(f"\nğŸ“¥ ä¸‹è½½ {config['name']}...")
                self.download_dataset(dataset_id)
                print(f"âœ… {config['name']} ä¸‹è½½å®Œæˆ")
            except Exception as e:
                print(f"âŒ {config['name']} ä¸‹è½½å¤±è´¥: {e}")
                # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†
                self.create_mock_dataset(dataset_id, config)
                print(f"ğŸ”„ å·²åˆ›å»º {config['name']} çš„æ¨¡æ‹Ÿæ•°æ®")
    
    def download_dataset(self, dataset_id: str):
        """ä¸‹è½½æŒ‡å®šæ•°æ®é›†"""
        if dataset_id not in self.datasets_config:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_id}")
        
        config = self.datasets_config[dataset_id]
        dataset_dir = self.data_dir / "raw" / dataset_id
        dataset_dir.mkdir(exist_ok=True, parents=True)
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½
        marker_file = dataset_dir / ".download_complete"
        if marker_file.exists():
            print(f"ğŸ“‹ {config['name']} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            return
        
        # ä¸‹è½½æ•°æ®é›†
        try:
            self._download_from_url(config["url"], dataset_dir, config)
        except Exception as e:
            print(f"âš ï¸ ä¸»é“¾æ¥ä¸‹è½½å¤±è´¥: {e}")
            if "backup_url" in config:
                print("ğŸ”„ å°è¯•å¤‡ç”¨é“¾æ¥...")
                self._download_from_url(config["backup_url"], dataset_dir, config)
            else:
                raise
        
        # æ ‡è®°ä¸‹è½½å®Œæˆ
        marker_file.write_text(f"Downloaded at: {os.getcwd()}\n")
    
    def _download_from_url(self, url: str, target_dir: Path, config: Dict):
        """ä»URLä¸‹è½½æ–‡ä»¶"""
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            # è·å–æ–‡ä»¶å
            filename = url.split("/")[-1]
            if not filename.endswith((".zip", ".tar.gz", ".json", ".csv")):
                filename = f"{config['name']}.zip"
            
            file_path = target_dir / filename
            
            # ä¸‹è½½æ–‡ä»¶
            total_size = int(response.headers.get('content-length', 0))
            with open(file_path, 'wb') as f, tqdm(
                desc=filename,
                total=total_size,
                unit="B",
                unit_scale=True
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
            
            # è§£å‹ç¼©ï¼ˆå¦‚æœæ˜¯å‹ç¼©æ–‡ä»¶ï¼‰
            if filename.endswith('.zip'):
                print("ğŸ“¦ è§£å‹ç¼©æ–‡ä»¶...")
                with zipfile.ZipFile(file_path, 'r') as zip_ref:
                    zip_ref.extractall(target_dir)
                file_path.unlink()  # åˆ é™¤å‹ç¼©åŒ…
            
            print(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")
            
        except requests.RequestException as e:
            print(f"âŒ ç½‘ç»œä¸‹è½½å¤±è´¥: {e}")
            raise
    
    def create_mock_dataset(self, dataset_id: str, config: Dict):
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†"""
        print(f"ğŸ”„ åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†: {config['name']}")
        
        dataset_dir = self.data_dir / "raw" / dataset_id
        dataset_dir.mkdir(exist_ok=True, parents=True)
        
        if dataset_id == "mcfend":
            self._create_mcfend_mock(dataset_dir, config)
        elif dataset_id == "weibo_rumors":
            self._create_weibo_mock(dataset_dir, config)
        
        # æ ‡è®°ä¸ºæ¨¡æ‹Ÿæ•°æ®
        marker_file = dataset_dir / ".mock_dataset"
        marker_file.write_text("This is mock data for development\n")
    
    def _create_mcfend_mock(self, dataset_dir: Path, config: Dict):
        """åˆ›å»ºMCFENDæ¨¡æ‹Ÿæ•°æ®"""
        samples = []
        
        # çœŸå®æ ·ä¾‹æ•°æ®
        fake_news_samples = [
            "æœˆå…¥10ä¸‡çš„æŠ•èµ„ç§˜è¯€ï¼Œä¿è¯æ”¶ç›Šï¼Œæ— é£é™©ï¼ç«‹å³åŠ å¾®ä¿¡äº†è§£è¯¦æƒ…ï¼",
            "ç¥–ä¼ ç§˜æ–¹åŒ…æ²»ç™¾ç—…ï¼Œä¸‰å¤©è§æ•ˆï¼ŒåŒ»é™¢ä¸å‘Šè¯‰ä½ çš„ç§˜å¯†ï¼",
            "é™æ—¶å…è´¹é¢†å–iPhone15ï¼Œç‚¹èµå…³æ³¨å³å¯å‚ä¸ï¼Œä»…é™ä»Šå¤©ï¼",
            "ç¥å¥‡ä¿å¥å“å»¶å¹´ç›Šå¯¿ï¼Œç§‘å­¦é™¢è®¤è¯ï¼Œè¯ºè´å°”å¥–æŠ€æœ¯ï¼",
            "åŒºå—é“¾æŒ–çŸ¿æœˆèµš50ä¸‡ï¼Œå†…å¹•æ¶ˆæ¯ï¼Œé”™è¿‡åæ‚”ç»ˆèº«ï¼"
        ]
        
        real_news_samples = [
            "å›½å®¶å‘å¸ƒæœ€æ–°å…»è€é‡‘è°ƒæ•´æ”¿ç­–ï¼Œé¢„è®¡ä¸Šè°ƒ3.5%ï¼Œå…·ä½“æ–¹æ¡ˆä¸‹æœˆå…¬å¸ƒã€‚",
            "ä¸‰ç”²åŒ»é™¢ä¸“å®¶æé†’ï¼šè€å¹´äººè¦æ³¨æ„åˆç†é¥®é£Ÿï¼Œé€‚å½“è¿åŠ¨ã€‚",
            "å¤®è¡Œå‘å¸ƒé€šçŸ¥ï¼šæ­£è§„æŠ•èµ„æ¸ é“é£é™©æç¤ºï¼Œè¯·é€šè¿‡æ­£è§„æœºæ„ç†è´¢ã€‚",
            "å¥åº·ç§‘æ™®ï¼šä¿å¥å“ä¸èƒ½æ›¿ä»£è¯ç‰©ï¼Œæœ‰ç—…è¯·åˆ°æ­£è§„åŒ»é™¢å°±è¯Šã€‚",
            "æ¶ˆè´¹è€…åä¼šæé†’ï¼šè°¨é˜²ç”µä¿¡è¯ˆéª—ï¼Œä¸è¦è½»ä¿¡é«˜æ”¶ç›ŠæŠ•èµ„ã€‚"
        ]
        
        # ç”Ÿæˆæ ·æœ¬
        for i in range(1000):  # ç”Ÿæˆ1000ä¸ªæ ·æœ¬ç”¨äºæµ‹è¯•
            if i % 2 == 0:  # å‡æ–°é—»
                text = fake_news_samples[i % len(fake_news_samples)]
                label = "fake"
                category = ["financial", "medical", "marketing"][i % 3]
            else:  # çœŸæ–°é—»
                text = real_news_samples[i % len(real_news_samples)]
                label = "real"
                category = "news"
            
            sample = {
                "id": f"mcfend_{i:06d}",
                "text": text,
                "label": label,
                "category": category,
                "source": ["weibo", "wechat", "douyin"][i % 3],
                "timestamp": "2024-01-01T00:00:00Z",
                "image": f"image_{i}.jpg" if i % 3 == 0 else None,
                "user_verified": i % 5 == 0,
                "engagement": {
                    "likes": i * 100,
                    "comments": i * 20,
                    "shares": i * 10
                }
            }
            samples.append(sample)
        
        # ä¿å­˜æ•°æ®
        output_file = dataset_dir / "mcfend_data.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(samples, f, ensure_ascii=False, indent=2)
        
        # åˆ›å»ºå…ƒæ•°æ®
        metadata = {
            "dataset_id": "mcfend",
            "name": config["name"],
            "total_samples": len(samples),
            "fake_samples": sum(1 for s in samples if s["label"] == "fake"),
            "real_samples": sum(1 for s in samples if s["label"] == "real"),
            "categories": ["financial", "medical", "marketing", "news"],
            "created_at": "2024-01-01T00:00:00Z",
            "version": "mock_v1.0"
        }
        
        with open(dataset_dir / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š MCFENDæ¨¡æ‹Ÿæ•°æ®é›†å·²åˆ›å»º: {len(samples)} ä¸ªæ ·æœ¬")
    
    def _create_weibo_mock(self, dataset_dir: Path, config: Dict):
        """åˆ›å»ºå¾®åšæ¨¡æ‹Ÿæ•°æ®"""
        samples = []
        
        rumor_samples = [
            "æŸåœ°å‘ç”Ÿåœ°éœ‡ï¼Œæ­»äº¡äººæ•°è¶…è¿‡1000äººï¼ˆæœªç»è¯å®ï¼‰",
            "æ–°å† ç—…æ¯’æ¥è‡ªå®éªŒå®¤æ³„éœ²ï¼ˆé˜´è°‹è®ºï¼‰",
            "æŸæ˜æ˜Ÿç§ç”Ÿæ´»ä¼ é—»ï¼ˆè°£è¨€ï¼‰",
            "é£Ÿç”¨æŸç§é£Ÿç‰©å¯ä»¥é¢„é˜²ç™Œç—‡ï¼ˆå¤¸å¤§å®£ä¼ ï¼‰"
        ]
        
        normal_samples = [
            "ä»Šæ—¥å¤©æ°”é¢„æŠ¥ï¼šæ™´è½¬å¤šäº‘ï¼Œæ°”æ¸©15-25åº¦",
            "å®˜æ–¹å‘å¸ƒï¼šç–«æƒ…é˜²æ§æªæ–½è°ƒæ•´é€šçŸ¥",
            "æ•™è‚²éƒ¨é€šçŸ¥ï¼šä¸­å°å­¦è¯¾ç¨‹å®‰æ’å˜æ›´",
            "äº¤é€šéƒ¨é—¨ï¼šåœ°é“çº¿è·¯ä¸´æ—¶è°ƒæ•´å…¬å‘Š"
        ]
        
        for i in range(500):  # ç”Ÿæˆ500ä¸ªæ ·æœ¬
            if i % 3 == 0:  # è°£è¨€
                text = rumor_samples[i % len(rumor_samples)]
                label = "rumor"
            else:  # æ­£å¸¸å†…å®¹
                text = normal_samples[i % len(normal_samples)]
                label = "non-rumor"
            
            sample = {
                "id": f"weibo_{i:06d}",
                "text": text,
                "label": label,
                "user": {
                    "verified": i % 4 == 0,
                    "followers_count": i * 1000,
                    "friends_count": i * 100
                },
                "created_at": "2024-01-01T00:00:00Z",
                "repost_count": i * 10,
                "comment_count": i * 5,
                "attitude_count": i * 50
            }
            samples.append(sample)
        
        # ä¿å­˜æ•°æ®
        output_file = dataset_dir / "weibo_data.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(samples, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š å¾®åšæ¨¡æ‹Ÿæ•°æ®é›†å·²åˆ›å»º: {len(samples)} ä¸ªæ ·æœ¬")
    
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        directories = [
            "data/raw",
            "data/processed", 
            "data/metadata",
            "models/pretrained",
            "models/finetuned",
            "models/checkpoints",
            "logs/training",
            "logs/inference",
            "uploads",
            "cache"
        ]
        
        for dir_path in directories:
            full_path = self.data_dir.parent / dir_path
            full_path.mkdir(exist_ok=True, parents=True)
            print(f"ğŸ“ åˆ›å»ºç›®å½•: {dir_path}")
    
    def verify_datasets(self):
        """éªŒè¯æ•°æ®é›†å®Œæ•´æ€§"""
        print("\nğŸ” éªŒè¯æ•°æ®é›†å®Œæ•´æ€§...")
        
        for dataset_id, config in self.datasets_config.items():
            dataset_dir = self.data_dir / "raw" / dataset_id
            
            if not dataset_dir.exists():
                print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_id}")
                continue
            
            data_files = list(dataset_dir.glob("*.json")) + list(dataset_dir.glob("*.csv"))
            
            if not data_files:
                print(f"âŒ æ•°æ®é›†æ–‡ä»¶ç¼ºå¤±: {dataset_id}")
                continue
            
            # æ£€æŸ¥æ•°æ®æ–‡ä»¶
            try:
                if data_files[0].suffix == ".json":
                    with open(data_files[0], 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        sample_count = len(data) if isinstance(data, list) else 1
                else:
                    import pandas as pd
                    df = pd.read_csv(data_files[0])
                    sample_count = len(df)
                
                print(f"âœ… {config['name']}: {sample_count} æ ·æœ¬")
                
            except Exception as e:
                print(f"âš ï¸ {config['name']} éªŒè¯å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¼€å§‹ä¸‹è½½å’Œé…ç½®æ•°æ®é›†...")
    
    downloader = DatasetDownloader()
    
    # 1. åˆ›å»ºç›®å½•ç»“æ„
    downloader.setup_directories()
    
    # 2. ä¸‹è½½æ•°æ®é›†
    downloader.download_all()
    
    # 3. éªŒè¯æ•°æ®é›†
    downloader.verify_datasets()
    
    print("\nğŸ‰ æ•°æ®é›†ä¸‹è½½å’Œé…ç½®å®Œæˆï¼")
    print("ğŸ“ æ•°æ®å­˜å‚¨ä½ç½®: ./data/")
    print("ğŸ“Š å¯ç”¨æ•°æ®é›†:")
    for dataset_id, config in downloader.datasets_config.items():
        print(f"   - {dataset_id}: {config['name']}")


if __name__ == "__main__":
    main()
