"""
AIæ¨¡å‹ä¸‹è½½è„šæœ¬
è‡ªåŠ¨ä¸‹è½½ChatGLMã€BERTã€LLaMAç­‰é¢„è®­ç»ƒæ¨¡å‹
"""

import os
import json
import requests
from pathlib import Path
from typing import Dict, List
import subprocess
import sys


class ModelDownloader:
    """AIæ¨¡å‹ä¸‹è½½å™¨"""
    
    def __init__(self, models_dir: str = "./models"):
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(exist_ok=True, parents=True)
        
        # æ¨¡å‹é…ç½®
        self.models_config = {
            "chatglm-6b": {
                "name": "ChatGLM-6B",
                "description": "æ¸…åå¤§å­¦å¼€æºçš„ä¸­æ–‡å¯¹è¯æ¨¡å‹",
                "source": "THUDM/chatglm-6b",
                "size_gb": 12.5,
                "type": "transformers",
                "tasks": ["text-generation", "classification"],
                "quantized_versions": ["int8", "int4"]
            },
            "chinese-bert": {
                "name": "Chinese-BERT-wwm-ext", 
                "description": "ä¸­æ–‡BERTé¢„è®­ç»ƒæ¨¡å‹",
                "source": "hfl/chinese-bert-wwm-ext",
                "size_gb": 1.2,
                "type": "transformers",
                "tasks": ["text-classification", "feature-extraction"]
            },
            "chinese-llama-7b": {
                "name": "Chinese-LLaMA-7B",
                "description": "ä¸­æ–‡LLaMAå¤§è¯­è¨€æ¨¡å‹", 
                "source": "ziqingyang/chinese-llama-7b",
                "size_gb": 15.0,
                "type": "transformers",
                "tasks": ["text-generation", "classification"]
            }
        }
        
        print("ğŸ¤– AIæ¨¡å‹ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def check_huggingface_cli(self):
        """æ£€æŸ¥huggingface-hub CLIæ˜¯å¦å®‰è£…"""
        try:
            result = subprocess.run(
                [sys.executable, "-c", "import huggingface_hub"],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                print("âœ… Hugging Face Hub å·²å®‰è£…")
                return True
        except:
            pass
        
        print("ğŸ“¦ å®‰è£… Hugging Face Hub...")
        subprocess.run([
            sys.executable, "-m", "pip", "install", 
            "huggingface-hub", "transformers", "--quiet"
        ])
        return True
    
    def download_model(self, model_id: str, quantized: bool = False):
        """ä¸‹è½½æŒ‡å®šæ¨¡å‹"""
        if model_id not in self.models_config:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_id}")
        
        config = self.models_config[model_id]
        model_dir = self.models_dir / model_id
        
        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        marker_file = model_dir / ".download_complete"
        if marker_file.exists():
            print(f"ğŸ“‹ {config['name']} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            return model_dir
        
        print(f"ğŸ“¥ ä¸‹è½½ {config['name']}...")
        print(f"ğŸ“Š é¢„è®¡å¤§å°: {config['size_gb']} GB")
        
        try:
            # ä½¿ç”¨huggingface-hubä¸‹è½½
            from huggingface_hub import snapshot_download
            
            model_path = snapshot_download(
                repo_id=config["source"],
                local_dir=model_dir,
                local_dir_use_symlinks=False,
                resume_download=True
            )
            
            # æ ‡è®°ä¸‹è½½å®Œæˆ
            marker_file.write_text(f"Downloaded from: {config['source']}\n")
            
            print(f"âœ… {config['name']} ä¸‹è½½å®Œæˆ")
            return model_dir
            
        except Exception as e:
            print(f"âŒ {config['name']} ä¸‹è½½å¤±è´¥: {e}")
            # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹é…ç½®
            self._create_mock_model(model_dir, config)
            return model_dir
    
    def _create_mock_model(self, model_dir: Path, config: Dict):
        """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹é…ç½®"""
        model_dir.mkdir(exist_ok=True, parents=True)
        
        # åˆ›å»ºæ¨¡å‹é…ç½®æ–‡ä»¶
        model_config = {
            "model_type": config["type"],
            "name": config["name"],
            "description": config["description"],
            "tasks": config["tasks"],
            "size_gb": config["size_gb"],
            "source": config["source"],
            "status": "mock",
            "created_at": "2024-01-01T00:00:00Z",
            "note": "è¿™æ˜¯æ¨¡æ‹Ÿæ¨¡å‹ï¼Œç”¨äºå¼€å‘æµ‹è¯•"
        }
        
        with open(model_dir / "config.json", 'w', encoding='utf-8') as f:
            json.dump(model_config, f, ensure_ascii=False, indent=2)
        
        # åˆ›å»ºæ¨¡æ‹Ÿæƒé‡æ–‡ä»¶ï¼ˆç©ºæ–‡ä»¶ï¼‰
        (model_dir / "pytorch_model.bin").touch()
        (model_dir / "tokenizer.json").touch()
        (model_dir / "tokenizer_config.json").touch()
        
        # æ ‡è®°ä¸ºæ¨¡æ‹Ÿæ¨¡å‹
        marker_file = model_dir / ".mock_model"
        marker_file.write_text("This is a mock model for development\n")
        
        print(f"ğŸ”„ å·²åˆ›å»º {config['name']} çš„æ¨¡æ‹Ÿé…ç½®")
    
    def download_all(self, include_large_models: bool = False):
        """ä¸‹è½½æ‰€æœ‰æ¨¡å‹"""
        print("ğŸ¤– å¼€å§‹ä¸‹è½½AIæ¨¡å‹...")
        
        # é¦–å…ˆæ£€æŸ¥ç¯å¢ƒ
        if not self.check_huggingface_cli():
            print("âŒ æ— æ³•å®‰è£…Hugging Faceä¾èµ–")
            include_large_models = False
        
        models_to_download = ["chinese-bert"]  # ä»å°æ¨¡å‹å¼€å§‹
        
        if include_large_models:
            models_to_download.extend(["chatglm-6b", "chinese-llama-7b"])
        else:
            print("âš ï¸ è·³è¿‡å¤§æ¨¡å‹ä¸‹è½½ï¼Œä»…ä¸‹è½½è½»é‡çº§æ¨¡å‹")
        
        for model_id in models_to_download:
            try:
                self.download_model(model_id)
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model_id} ä¸‹è½½å¤±è´¥: {e}")
        
        print("ğŸ‰ æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
    
    def list_downloaded_models(self):
        """åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹"""
        print("\nğŸ“‹ å·²ä¸‹è½½çš„æ¨¡å‹:")
        
        for model_id, config in self.models_config.items():
            model_dir = self.models_dir / model_id
            
            if model_dir.exists():
                if (model_dir / ".mock_model").exists():
                    status = "ğŸ”„ æ¨¡æ‹Ÿæ¨¡å‹"
                elif (model_dir / ".download_complete").exists():
                    status = "âœ… å®Œæ•´ä¸‹è½½"
                else:
                    status = "âš ï¸ ä¸å®Œæ•´"
                
                print(f"   - {config['name']}: {status}")
            else:
                print(f"   - {config['name']}: âŒ æœªä¸‹è½½")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="AIæ¨¡å‹ä¸‹è½½å·¥å…·")
    parser.add_argument("--all", action="store_true", help="ä¸‹è½½æ‰€æœ‰æ¨¡å‹ï¼ˆåŒ…æ‹¬å¤§æ¨¡å‹ï¼‰")
    parser.add_argument("--model", type=str, help="ä¸‹è½½æŒ‡å®šæ¨¡å‹")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹")
    parser.add_argument("--quantized", action="store_true", help="ä¸‹è½½é‡åŒ–ç‰ˆæœ¬")
    
    args = parser.parse_args()
    
    downloader = ModelDownloader()
    
    if args.list:
        print("ğŸ¤– å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
        for model_id, config in downloader.models_config.items():
            print(f"   - {model_id}: {config['name']} ({config['size_gb']} GB)")
        return
    
    if args.model:
        downloader.download_model(args.model, args.quantized)
    elif args.all:
        downloader.download_all(include_large_models=True)
    else:
        # é»˜è®¤åªä¸‹è½½è½»é‡çº§æ¨¡å‹
        downloader.download_all(include_large_models=False)
    
    # æ˜¾ç¤ºä¸‹è½½ç»“æœ
    downloader.list_downloaded_models()


if __name__ == "__main__":
    main()
