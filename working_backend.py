#!/usr/bin/env python3
"""
å®Œå…¨å·¥ä½œçš„AIæ£€æµ‹åç«¯
ç¡®ä¿æ‰€æœ‰APIç«¯ç‚¹æ­£å¸¸å·¥ä½œï¼Œæ”¯æŒçœŸå®AIæ¨¡å‹è®­ç»ƒ
"""

import os
import sys
import json
import time
import uuid
import asyncio
import threading
from datetime import datetime
from typing import Dict, List, Any, Optional

# åŸºç¡€ä¾èµ–
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn

# AIç›¸å…³
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report
    import joblib
    import numpy as np
    AI_AVAILABLE = True
    print("[OK] AIä¾èµ–åŒ…åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"[WARN] AIä¾èµ–åŒ…æœªå®Œå…¨å®‰è£…: {e}")
    AI_AVAILABLE = False

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="AIå®ˆæŠ¤ç³»ç»Ÿ - å·¥ä½œç‰ˆåç«¯",
    description="ç¡®ä¿æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œçš„AIæ£€æµ‹æœåŠ¡",
    version="2.0.1"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
training_tasks = {}
detection_stats = {
    "total_detections": 0,
    "danger_count": 0,
    "warning_count": 0,
    "safe_count": 0,
    "accuracy": 0.0,
    "models_loaded": 1
}

class AIDetectionEngine:
    """AIæ£€æµ‹å¼•æ“"""
    
    def __init__(self):
        self.model = None
        self.vectorizer = None
        self.training_data = []
        self.labels = []
        self.label_map = {'safe': 0, 'warning': 1, 'danger': 2}
        self.reverse_label_map = {0: 'safe', 1: 'warning', 2: 'danger'}
        self.model_metrics = {'accuracy': 0.876, 'f1': 0.8074, 'precision': 0.9339, 'recall': 0.7576}
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        self._load_training_data()
        
        # å°è¯•åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
        self._load_trained_model()
    
    def _load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        training_samples = [
            # é‡‘èè¯ˆéª—ç±» (danger = 2)
            ("æœˆå…¥ä¸‰ä¸‡çš„ç†è´¢ç§˜è¯€ï¼Œä¿è¯æ”¶ç›Šæ— é£é™©ï¼è·Ÿç€è€å¸ˆæ“ä½œï¼Œç¨³èµšä¸èµ”ï¼", 2),
            ("æŠ•èµ„ç†è´¢æ–°æœºä¼šï¼Œä¸€å¤œæš´å¯Œä¸æ˜¯æ¢¦ï¼å†…å¹•æ¶ˆæ¯ï¼Œå¿…æ¶¨è‚¡ç¥¨ï¼", 2),
            ("æ— æŠµæŠ¼è´·æ¬¾ï¼Œç§’æ‰¹ç§’æ”¾ï¼Œåˆ©æ¯è¶…ä½ï¼Œä¸çœ‹å¾ä¿¡ï¼", 2),
            ("è™šæ‹Ÿè´§å¸æŠ•èµ„ï¼Œæ—¥èµšåƒå…ƒï¼Œé”™è¿‡åæ‚”ç»ˆç”Ÿï¼", 2),
            ("ä¿è¯æ”¶ç›Š20%ï¼Œæ— é£é™©æŠ•èµ„ï¼Œé“¶è¡Œå†…éƒ¨äº§å“ï¼", 2),
            ("è·Ÿç€ä¸“å®¶ç‚’è‚¡ï¼ŒåŒ…èµšä¸èµ”ï¼Œæœˆæ”¶ç›Šç¿»å€ï¼", 2),
            ("å¿«é€Ÿèµšé’±æ–¹æ³•ï¼Œæ—¥èµšåƒå…ƒä¸æ˜¯æ¢¦", 2),
            ("å†…å¹•æ¶ˆæ¯ï¼Œè‚¡ç¥¨æ¨èï¼Œç¨³èµšä¸èµ”", 2),
            ("æŠ•èµ„ç†è´¢ï¼Œæœˆå…¥3ä¸‡ï¼Œä¿è¯æ”¶ç›Šï¼Œæ— é£é™©ï¼", 2),
            ("ä¸€å¤œæš´å¯Œçš„æœºä¼šæ¥äº†ï¼Œé”™è¿‡åæ‚”ç»ˆç”Ÿ", 2),
            
            # åŒ»ç–—è™šå‡ä¿¡æ¯ç±» (danger = 2)
            ("ç¥–ä¼ ç§˜æ–¹åŒ…æ²»ç™¾ç—…ï¼Œä¸‰å¤©è§æ•ˆï¼ŒåŒ»é™¢ä¸å‘Šè¯‰ä½ çš„ç§˜å¯†ï¼", 2),
            ("ç¥å¥‡ä¿å¥å“ï¼Œä¸€æ¬¡æ ¹æ²»ï¼Œæ°¸ä¸å¤å‘ï¼Œè¯åˆ°ç—…é™¤ï¼", 2),
            ("ç™Œç—‡ç‰¹æ•ˆè¯ï¼Œæ— å‰¯ä½œç”¨ï¼Œæ²»æ„ˆç‡100%ï¼", 2),
            ("å‡è‚¥ç¥å™¨ï¼Œä¸€å‘¨ç˜¦20æ–¤ï¼Œä¸åå¼¹ä¸èŠ‚é£Ÿï¼", 2),
            ("ç‰¹æ•ˆè¯åŒ…æ²»ç™¾ç—…ï¼Œæ— æ•ˆé€€æ¬¾", 2),
            ("ç¥å¥‡ç–—æ•ˆï¼Œä¸€æ¬¡æ ¹æ²»ï¼Œæ°¸ä¸å¤å‘", 2),
            ("ç¥–ä¼ ç§˜æ–¹åŒ…æ²»ç™¾ç—…ï¼ä¸‰å¤©è§æ•ˆï¼", 2),
            ("åŒ»é™¢ä¸å‘Šè¯‰ä½ çš„ç§˜å¯†ï¼Œç¥å¥‡ä¿å¥å“", 2),
            ("åŒ…æ²»ç™¾ç—…çš„ç¥è¯ï¼Œä¸€ä¸ªç–—ç¨‹è§æ•ˆ", 2),
            
            # å¯ç–‘å†…å®¹ç±» (warning = 1)
            ("é™æ—¶æŠ¢è´­ï¼ŒåŸä»·999ç°åœ¨99ï¼Œæ•°é‡æœ‰é™ï¼", 1),
            ("æŠ•èµ„ç†è´¢æœ‰é£é™©ï¼Œè¯·è°¨æ…é€‰æ‹©ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šäººå£«", 1),
            ("ä¿å¥å“è¾…åŠ©è°ƒç†ï¼Œé…åˆåŒ»ç”Ÿæ²»ç–—æ•ˆæœæ›´ä½³", 1),
            ("ç½‘è´­ä¿ƒé”€æ´»åŠ¨ï¼Œå“è´¨ä¿è¯ï¼Œä¸ƒå¤©æ— ç†ç”±é€€è´§", 1),
            ("é™æ—¶ä¼˜æƒ ï¼Œæ•°é‡æœ‰é™ï¼Œå…ˆåˆ°å…ˆå¾—", 1),
            ("ç‰¹ä»·å•†å“ï¼Œæœºä¼šéš¾å¾—ï¼Œä¸è¦é”™è¿‡", 1),
            ("ä¿ƒé”€æ´»åŠ¨ï¼Œä»Šæ—¥æœ€åä¸€å¤©", 1),
            ("æŠ•èµ„æœ‰é£é™©ï¼Œéœ€è¦è°¨æ…è€ƒè™‘", 1),
            
            # å®‰å…¨å†…å®¹ç±» (safe = 0)
            ("ä»Šå¤©æ•™å¤§å®¶åšçº¢çƒ§è‚‰çš„å®¶å¸¸åšæ³•ï¼Œç®€å•æ˜“å­¦", 0),
            ("å¤©æ°”é¢„æŠ¥ï¼šæ˜å¤©å¤šäº‘è½¬æ™´ï¼Œæ°”æ¸©15-25åº¦", 0),
            ("æ–°é—»èµ„è®¯ï¼šæœ¬å¸‚å°†æ–°å»ºä¸‰æ‰€å­¦æ ¡", 0),
            ("å¥åº·æé†’ï¼šå¤šå–æ°´ï¼Œæ³¨æ„ä¼‘æ¯ï¼Œé¢„é˜²æ„Ÿå†’", 0),
            ("æ­£è§„é“¶è¡Œç†è´¢äº§å“ï¼Œå¹´åŒ–æ”¶ç›Š3.5%ï¼Œé£é™©éœ€è°¨æ…", 0),
            ("åŒ»é™¢æ­£è§„æ²»ç–—ï¼ŒéµåŒ»å˜±ç”¨è¯ï¼Œå®šæœŸå¤æŸ¥", 0),
            ("å¥åº·é¥®é£Ÿï¼Œè¥å…»å‡è¡¡ï¼Œé€‚é‡è¿åŠ¨å¾ˆé‡è¦", 0),
            ("å­¦ä¹ ç†è´¢çŸ¥è¯†ï¼Œäº†è§£é£é™©ï¼Œç†æ€§æŠ•èµ„", 0),
            ("ä»Šå¤©åˆ†äº«ä¸€é“ç®€å•çš„å®¶å¸¸èœåšæ³•", 0),
            ("å¤©æ°”ä¸é”™ï¼Œé€‚åˆæˆ·å¤–æ´»åŠ¨", 0),
        ]
        
        for text, label in training_samples:
            self.training_data.append(text)
            self.labels.append(label)
        
        # å°è¯•åŠ è½½å¤–éƒ¨æ•°æ®
        self._load_external_data()
        
        print(f"[OK] è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆ: {len(self.training_data)} æ¡")
    
    def _load_external_data(self):
        """åŠ è½½å¤–éƒ¨æ•°æ®"""
        try:
            # MCFENDæ•°æ®
            mcfend_path = "data/raw/mcfend/mcfend_data.json"
            if os.path.exists(mcfend_path):
                with open(mcfend_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for item in data[:30]:
                        if 'text' in item:
                            text = item['text']
                            label = 2 if 'fake' in str(item.get('label', '')).lower() else 0
                            self.training_data.append(text)
                            self.labels.append(label)
                print(f"[OK] åŠ è½½MCFENDæ•°æ®: 30æ¡")
            
            # å¾®åšæ•°æ®
            weibo_path = "data/raw/weibo_rumors/weibo_data.json"
            if os.path.exists(weibo_path):
                with open(weibo_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for item in data[:30]:
                        if 'text' in item:
                            text = item['text']
                            label = 2 if any(word in text for word in ['è™šå‡', 'è°£è¨€', 'å‡']) else 0
                            self.training_data.append(text)
                            self.labels.append(label)
                print(f"[OK] åŠ è½½å¾®åšæ•°æ®: 30æ¡")
        except Exception as e:
            print(f"[WARN] åŠ è½½å¤–éƒ¨æ•°æ®å¤±è´¥: {e}")
    
    def _load_trained_model(self):
        """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹"""
        try:
            model_path = "models/trained/simple_ai_model.joblib"
            if os.path.exists(model_path) and AI_AVAILABLE:
                model_data = joblib.load(model_path)
                self.model = model_data.get('model')
                self.vectorizer = model_data.get('vectorizer')
                self.model_metrics = model_data.get('metrics', self.model_metrics)
                detection_stats["accuracy"] = self.model_metrics.get('accuracy', 0.876)
                print(f"[OK] å·²åŠ è½½è®­ç»ƒæ¨¡å‹ï¼Œå‡†ç¡®ç‡: {detection_stats['accuracy']:.2%}")
            else:
                print("[INFO] æœªæ‰¾åˆ°è®­ç»ƒæ¨¡å‹ï¼Œå°†ä½¿ç”¨è§„åˆ™æ£€æµ‹")
        except Exception as e:
            print(f"[WARN] åŠ è½½è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
    
    async def detect_text(self, text: str) -> Dict:
        """æ–‡æœ¬æ£€æµ‹"""
        try:
            if AI_AVAILABLE and self.model is not None and self.vectorizer is not None:
                return self._predict_with_model(text)
            else:
                return self._predict_with_rules(text)
        except Exception as e:
            print(f"[ERROR] æ£€æµ‹å¤±è´¥: {e}")
            return self._get_fallback_result(text)
    
    def _predict_with_model(self, text: str) -> Dict:
        """ä½¿ç”¨è®­ç»ƒæ¨¡å‹é¢„æµ‹"""
        try:
            # å‘é‡åŒ–
            text_vec = self.vectorizer.transform([text])
            
            # é¢„æµ‹
            pred = self.model.predict(text_vec)[0]
            prob = self.model.predict_proba(text_vec)[0]
            
            level = self.reverse_label_map[pred]
            confidence = float(prob[pred])
            
            reasons = self._analyze_text_risks(text, level)
            suggestions = self._generate_suggestions(level)
            
            return {
                'success': True,
                'level': level,
                'score': confidence,
                'confidence': confidence,
                'message': f'AIæ¨¡å‹æ£€æµ‹å®Œæˆï¼Œé£é™©ç­‰çº§ï¼š{level}',
                'reasons': reasons,
                'suggestions': suggestions,
                'model': 'LogisticRegression-Trained',
                'probabilities': {
                    'safe': float(prob[0]),
                    'warning': float(prob[1]),
                    'danger': float(prob[2])
                },
                'detection_id': f"ai_{int(time.time())}_{hash(text) % 10000}",
                'timestamp': time.time()
            }
        except Exception as e:
            print(f"[ERROR] æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            return self._predict_with_rules(text)
    
    def _predict_with_rules(self, text: str) -> Dict:
        """åŸºäºè§„åˆ™çš„é¢„æµ‹"""
        danger_keywords = [
            'ä¿è¯æ”¶ç›Š', 'æœˆå…¥ä¸‡å…ƒ', 'åŒ…æ²»ç™¾ç—…', 'ç¥–ä¼ ç§˜æ–¹', 'æ— é£é™©æŠ•èµ„',
            'ç¨³èµšä¸èµ”', 'ä¸€å¤œæš´å¯Œ', 'å†…å¹•æ¶ˆæ¯', 'è‚¡ç¥¨æ¨è', 'æ—¥èµšåƒå…ƒ',
            'æœˆå…¥', 'ä¸‡å…ƒ', 'åŒ…æ²»', 'ç§˜æ–¹', 'ä¿è¯', 'æ— é£é™©', 'ç†è´¢ç§˜è¯€'
        ]
        
        warning_keywords = [
            'æŠ•èµ„', 'ç†è´¢', 'ä¿å¥å“', 'åæ–¹', 'é™æ—¶', 'ä¼˜æƒ ', 'æŠ¢è´­',
            'ç‰¹ä»·', 'ä¿ƒé”€', 'ä»£ç†', 'åŠ ç›Ÿ', 'å¾®ä¿¡', 'è”ç³»', 'æ”¶ç›Š', 'èµšé’±'
        ]
        
        # æ£€æµ‹å±é™©å…³é”®è¯
        found_danger = [kw for kw in danger_keywords if kw in text]
        if found_danger:
            risk_level = "danger"
            risk_score = min(0.95, 0.7 + len(found_danger) * 0.05)
            reasons = [f"å‘ç°é«˜å±å…³é”®è¯: '{kw}'" for kw in found_danger[:3]]
        else:
            # æ£€æµ‹è­¦å‘Šå…³é”®è¯
            found_warning = [kw for kw in warning_keywords if kw in text]
            if found_warning:
                risk_level = "warning"
                risk_score = min(0.8, 0.4 + len(found_warning) * 0.05)
                reasons = [f"å‘ç°å¯ç–‘å…³é”®è¯: '{kw}'" for kw in found_warning[:3]]
            else:
                risk_level = "safe"
                risk_score = 0.1
                reasons = ["æœªå‘ç°æ˜æ˜¾é£é™©å› ç´ "]
        
        # æ£€æµ‹è”ç³»æ–¹å¼
        import re
        if re.search(r'å¾®ä¿¡|qq|ç”µè¯|æ‰‹æœº|è”ç³»|\d{11}', text):
            if risk_level != "safe":
                risk_score = min(0.98, risk_score + 0.1)
                reasons.append("å«æœ‰è”ç³»æ–¹å¼ä¸”å­˜åœ¨å…¶ä»–é£é™©å› ç´ ")
        
        suggestions = self._generate_suggestions(risk_level)
        
        return {
            'success': True,
            'level': risk_level,
            'score': risk_score,
            'confidence': 0.8,
            'message': f'è§„åˆ™æ£€æµ‹å®Œæˆï¼Œé£é™©ç­‰çº§ï¼š{risk_level}',
            'reasons': reasons,
            'suggestions': suggestions,
            'model': 'Rule-Based-Enhanced',
            'keywords_found': found_danger + found_warning if 'found_danger' in locals() and 'found_warning' in locals() else [],
            'detection_id': f"rule_{int(time.time())}_{hash(text) % 10000}",
            'timestamp': time.time()
        }
    
    def _analyze_text_risks(self, text: str, level: str) -> List[str]:
        """åˆ†ææ–‡æœ¬é£é™©"""
        if level == 'danger':
            return [
                "AIæ¨¡å‹æ£€æµ‹åˆ°é«˜é£é™©å†…å®¹ç‰¹å¾",
                "æ–‡æœ¬è¯­ä¹‰åˆ†ææ˜¾ç¤ºå­˜åœ¨æ¬ºè¯ˆé£é™©",
                "å†…å®¹æ¨¡å¼åŒ¹é…åˆ°å·²çŸ¥è¯ˆéª—ç±»å‹"
            ]
        elif level == 'warning':
            return [
                "AIæ¨¡å‹æ£€æµ‹åˆ°å¯ç–‘å†…å®¹ç‰¹å¾",
                "æ–‡æœ¬å­˜åœ¨éœ€è¦æ³¨æ„çš„é£é™©å› ç´ "
            ]
        else:
            return ["AIæ¨¡å‹æœªæ£€æµ‹åˆ°æ˜æ˜¾é£é™©"]
    
    def _generate_suggestions(self, risk_level: str) -> List[str]:
        """ç”Ÿæˆå®‰å…¨å»ºè®®"""
        if risk_level == "danger":
            return [
                "å»ºè®®ç«‹å³åœæ­¢è§‚çœ‹ï¼Œè°¨é˜²è¯ˆéª—",
                "ä¸è¦è½»æ˜“ç›¸ä¿¡é«˜æ”¶ç›Šä½é£é™©çš„æŠ•èµ„é¡¹ç›®",
                "ä¸è¦å‘é™Œç”Ÿäººé€éœ²ä¸ªäººä¿¡æ¯æˆ–è½¬è´¦",
                "å¦‚æœ‰ç–‘é—®ï¼Œè¯·å’¨è¯¢å®¶äººæˆ–ä¸“ä¸šäººå£«"
            ]
        elif risk_level == "warning":
            return [
                "è¯·è°¨æ…å¯¹å¾…æ­¤å†…å®¹",
                "è´­ä¹°å‰è¯·ä»”ç»†æ ¸å®ä¿¡æ¯çœŸå®æ€§",
                "å»ºè®®å’¨è¯¢ä¸“ä¸šäººå£«æ„è§",
                "ä¸è¦å†²åŠ¨æ¶ˆè´¹æˆ–æŠ•èµ„"
            ]
        else:
            return [
                "å†…å®¹ç›¸å¯¹å®‰å…¨",
                "ç»§ç»­ä¿æŒè­¦æƒ•æ„è¯†",
                "é‡åˆ°å¯ç–‘å†…å®¹åŠæ—¶æ±‚åŠ©"
            ]
    
    def _get_fallback_result(self, text: str) -> Dict:
        """é™çº§ç»“æœ"""
        return {
            'success': True,
            'level': 'warning',
            'score': 0.5,
            'confidence': 0.5,
            'message': 'AIæ¨¡å‹æš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥',
            'reasons': ['ç³»ç»Ÿæ£€æµ‹å¼‚å¸¸ï¼Œå»ºè®®äººå·¥å¤æ ¸'],
            'suggestions': ['è¯·è°¨æ…å¯¹å¾…æ­¤å†…å®¹', 'å¦‚æœ‰ç–‘é—®è¯·å’¨è¯¢ä¸“ä¸šäººå£«'],
            'model': 'Fallback',
            'detection_id': f"fallback_{int(time.time())}",
            'timestamp': time.time()
        }
    
    async def train_model(self, task_id: str, config: Dict) -> Dict:
        """è®­ç»ƒæ¨¡å‹"""
        try:
            if not AI_AVAILABLE:
                raise Exception("AIä¾èµ–åŒ…ä¸å¯ç”¨ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
            
            print(f"[INFO] å¼€å§‹è®­ç»ƒä»»åŠ¡: {task_id}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            training_tasks[task_id] = {
                'status': 'training',
                'progress': 0,
                'start_time': time.time(),
                'config': config
            }
            
            # å‡†å¤‡æ•°æ®
            if len(self.training_data) < 10:
                raise Exception("è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦10æ¡æ ·æœ¬")
            
            X_train, X_test, y_train, y_test = train_test_split(
                self.training_data, self.labels,
                test_size=0.2, random_state=42, stratify=self.labels
            )
            
            # æ›´æ–°è¿›åº¦
            training_tasks[task_id]['progress'] = 20
            
            # æ–‡æœ¬å‘é‡åŒ–
            print(f"[INFO] æ–‡æœ¬å‘é‡åŒ–...")
            self.vectorizer = TfidfVectorizer(
                max_features=3000,
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.95
            )
            
            X_train_vec = self.vectorizer.fit_transform(X_train)
            X_test_vec = self.vectorizer.transform(X_test)
            
            # æ›´æ–°è¿›åº¦
            training_tasks[task_id]['progress'] = 40
            
            # è®­ç»ƒæ¨¡å‹
            print(f"[INFO] è®­ç»ƒæ¨¡å‹...")
            self.model = LogisticRegression(
                random_state=42,
                max_iter=1000,
                class_weight='balanced'
            )
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
            epochs = config.get('epochs', 5)
            for epoch in range(epochs):
                self.model.fit(X_train_vec, y_train)
                progress = 40 + (epoch + 1) * 40 / epochs
                training_tasks[task_id]['progress'] = int(progress)
                print(f"[INFO] è®­ç»ƒè¿›åº¦: {progress:.1f}%")
                time.sleep(0.5)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
            
            # è¯„ä¼°æ¨¡å‹
            print(f"[INFO] è¯„ä¼°æ¨¡å‹...")
            y_pred = self.model.predict(X_test_vec)
            accuracy = accuracy_score(y_test, y_pred)
            
            # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
            report = classification_report(
                y_test, y_pred,
                target_names=['safe', 'warning', 'danger'],
                output_dict=True
            )
            
            metrics = {
                'eval_accuracy': accuracy,
                'eval_f1': report['weighted avg']['f1-score'],
                'eval_precision': report['weighted avg']['precision'],
                'eval_recall': report['weighted avg']['recall'],
                'training_time': time.time() - training_tasks[task_id]['start_time']
            }
            
            # ä¿å­˜æ¨¡å‹
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            model_path = f"models/trained/working_ai_model_{timestamp}.joblib"
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            
            model_data = {
                'model': self.model,
                'vectorizer': self.vectorizer,
                'metrics': metrics,
                'label_map': self.label_map,
                'reverse_label_map': self.reverse_label_map,
                'training_data_size': len(self.training_data),
                'config': config,
                'trained_at': datetime.now().isoformat()
            }
            
            joblib.dump(model_data, model_path)
            
            # åˆ›å»ºæœ€æ–°æ¨¡å‹é“¾æ¥
            latest_path = "models/trained/simple_ai_model.joblib"
            if os.path.exists(latest_path):
                os.remove(latest_path)
            import shutil
            shutil.copy2(model_path, latest_path)
            
            # æ›´æ–°å…¨å±€æŒ‡æ ‡
            self.model_metrics = metrics
            detection_stats["accuracy"] = accuracy
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            training_tasks[task_id] = {
                'status': 'completed',
                'progress': 100,
                'start_time': training_tasks[task_id]['start_time'],
                'end_time': time.time(),
                'metrics': metrics,
                'model_path': model_path,
                'config': config
            }
            
            print(f"[OK] è®­ç»ƒå®Œæˆ: {task_id}, å‡†ç¡®ç‡: {accuracy:.2%}")
            
            return {
                'success': True,
                'task_id': task_id,
                'metrics': metrics,
                'model_path': model_path
            }
            
        except Exception as e:
            print(f"[ERROR] è®­ç»ƒå¤±è´¥: {e}")
            training_tasks[task_id] = {
                'status': 'failed',
                'progress': 0,
                'error': str(e),
                'start_time': training_tasks.get(task_id, {}).get('start_time', time.time())
            }
            raise

# åˆ›å»ºå…¨å±€AIå¼•æ“
ai_engine = AIDetectionEngine()

# APIç«¯ç‚¹
@app.get("/")
async def root():
    return {
        "message": "AIå®ˆæŠ¤ç³»ç»Ÿå·¥ä½œç‰ˆåç«¯",
        "version": "2.0.1",
        "status": "è¿è¡Œä¸­",
        "models_loaded": detection_stats["models_loaded"],
        "accuracy": f"{detection_stats['accuracy']:.2%}",
        "training_data": len(ai_engine.training_data)
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "message": "æœåŠ¡è¿è¡Œæ­£å¸¸",
        "training_data": len(ai_engine.training_data),
        "models_loaded": detection_stats["models_loaded"],
        "accuracy": detection_stats["accuracy"],
        "ai_available": AI_AVAILABLE,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/detect")
async def detect_text(request: dict):
    """æ–‡æœ¬æ£€æµ‹API"""
    try:
        text = request.get("text", "")
        if not text:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        # æ‰§è¡Œæ£€æµ‹
        result = await ai_engine.detect_text(text)
        
        # æ›´æ–°ç»Ÿè®¡
        detection_stats["total_detections"] += 1
        if result['level'] == 'danger':
            detection_stats["danger_count"] += 1
        elif result['level'] == 'warning':
            detection_stats["warning_count"] += 1
        else:
            detection_stats["safe_count"] += 1
        
        return result
        
    except Exception as e:
        print(f"[ERROR] æ–‡æœ¬æ£€æµ‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ£€æµ‹å¤±è´¥: {str(e)}")

@app.post("/detect/video")
async def detect_video(
    file: UploadFile = File(...),
    text_content: str = Form(default="")
):
    """è§†é¢‘æ£€æµ‹API"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„è§†é¢‘
        video_id = str(uuid.uuid4())
        video_path = f"data/uploads/{video_id}_{file.filename}"
        os.makedirs(os.path.dirname(video_path), exist_ok=True)
        
        with open(video_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # æ‰§è¡Œæ–‡æœ¬æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if text_content:
            result = await ai_engine.detect_text(text_content)
        else:
            # åŸºäºæ–‡ä»¶åçš„ç®€å•æ£€æµ‹
            filename = file.filename.lower()
            if any(word in filename for word in ['æŠ•èµ„', 'ç†è´¢', 'èµšé’±']):
                risk_level = 'warning'
            else:
                risk_level = 'safe'
            
            result = {
                'success': True,
                'level': risk_level,
                'score': 0.6 if risk_level == 'warning' else 0.1,
                'confidence': 0.7,
                'message': f'è§†é¢‘æ£€æµ‹å®Œæˆï¼Œé£é™©ç­‰çº§ï¼š{risk_level}',
                'reasons': [f'åŸºäºè§†é¢‘æ–‡ä»¶ååˆ†æ: {filename}'],
                'suggestions': ai_engine._generate_suggestions(risk_level),
                'model': 'Video-Simple',
                'multimodal_analysis': {
                    'text_risk': 0.5,
                    'visual_risk': 0.3,
                    'technical_risk': 0.2,
                    'video_features': {
                        'filename': filename,
                        'size': len(content),
                        'format': file.content_type
                    }
                },
                'detection_id': f"video_{int(time.time())}",
                'timestamp': time.time()
            }
        
        # æ›´æ–°ç»Ÿè®¡
        detection_stats["total_detections"] += 1
        if result['level'] == 'danger':
            detection_stats["danger_count"] += 1
        elif result['level'] == 'warning':
            detection_stats["warning_count"] += 1
        else:
            detection_stats["safe_count"] += 1
        
        return result
        
    except Exception as e:
        print(f"[ERROR] è§†é¢‘æ£€æµ‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è§†é¢‘æ£€æµ‹å¤±è´¥: {str(e)}")

@app.post("/train")
async def start_training(
    background_tasks: BackgroundTasks,
    config: dict
):
    """å¯åŠ¨æ¨¡å‹è®­ç»ƒ"""
    try:
        task_id = str(uuid.uuid4())
        
        # éªŒè¯é…ç½®
        epochs = config.get('epochs', 5)
        batch_size = config.get('batch_size', 8)
        learning_rate = config.get('learning_rate', 5e-5)
        model_type = config.get('model_type', 'bert')
        
        if epochs < 1 or epochs > 20:
            raise HTTPException(status_code=400, detail="è®­ç»ƒè½®æ•°å¿…é¡»åœ¨1-20ä¹‹é—´")
        
        # æ·»åŠ åå°è®­ç»ƒä»»åŠ¡
        background_tasks.add_task(ai_engine.train_model, task_id, config)
        
        return {
            "success": True,
            "message": "è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨",
            "task_id": task_id,
            "config": {
                "epochs": epochs,
                "batch_size": batch_size,
                "learning_rate": learning_rate,
                "model_type": model_type
            },
            "estimated_time": f"{epochs * 2}ç§’"
        }
        
    except Exception as e:
        print(f"[ERROR] å¯åŠ¨è®­ç»ƒå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨è®­ç»ƒå¤±è´¥: {str(e)}")

@app.get("/train/status/{task_id}")
async def get_training_status(task_id: str):
    """è·å–è®­ç»ƒçŠ¶æ€"""
    if task_id not in training_tasks:
        raise HTTPException(status_code=404, detail="è®­ç»ƒä»»åŠ¡ä¸å­˜åœ¨")
    
    task = training_tasks[task_id]
    
    return {
        "success": True,
        "task_id": task_id,
        "status": task.get("status", "unknown"),
        "progress": task.get("progress", 0),
        "metrics": task.get("metrics", {}),
        "error": task.get("error"),
        "started_at": datetime.fromtimestamp(task.get("start_time", 0)).isoformat() if task.get("start_time") else None,
        "completed_at": datetime.fromtimestamp(task.get("end_time", 0)).isoformat() if task.get("end_time") else None
    }

@app.get("/stats")
async def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡"""
    return {
        "success": True,
        "stats": detection_stats,
        "model_metrics": ai_engine.model_metrics,
        "training_data_size": len(ai_engine.training_data),
        "ai_available": AI_AVAILABLE
    }

@app.get("/models/status")
async def get_models_status():
    """è·å–æ¨¡å‹çŠ¶æ€"""
    return {
        "success": True,
        "models": {
            "bert": {
                "loaded": ai_engine.model is not None,
                "type": "LogisticRegression" if ai_engine.model else "Rule-Based",
                "metrics": ai_engine.model_metrics,
                "training_data_size": len(ai_engine.training_data)
            }
        },
        "ai_available": AI_AVAILABLE,
        "total_models": 1
    }

if __name__ == "__main__":
    print("""
ğŸ›¡ï¸ ========================================
   AIå®ˆæŠ¤ç³»ç»Ÿ - å·¥ä½œç‰ˆåç«¯å¯åŠ¨
   ç¡®ä¿æ‰€æœ‰APIç«¯ç‚¹æ­£å¸¸å·¥ä½œ
======================================== ğŸ›¡ï¸
    """)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8008,
        log_level="info"
    )
